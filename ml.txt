Machine Learning-

Oo

What Is Machine Learning-
In the real world, we are surrounded by humans who can learn everything from their
experiences with their learning capability, and we have computers or machines which work on
our instructions. But can a machine also learn from experiences or past data like a human does?
So here comes the role of Machine Learning.

Machine Learning is said as a subset of artificial intelligence that is mainly concerned with the
development of algorithms which allow a computer to learn from the data and past
experiences on their own. The term machine learning was first introduced by Arthur Samuel in
1959. We can define it in a summarized way as:

With the help of sample historical data, which is known as training data, machine learning
algorithms build a mathematical model that helps in making predictions or decisions without
being explicitly programmed. Machine learning brings computer science and statistics together
for creating predictive models. Machine learning constructs or uses the algorithms that learn
from historical data. The more we will provide the information, the higher will be performance.

0)

Inputpast

How Does Machine Learning Works..??

A Machine Learning system learns from historical data, builds the prediction models, and
whenever it receives new data, predicts the output for it. The accuracy of predicted output
depends upon the amount of data, as the huge amount of data helps to build a better model
which predicts the output more accurately.

Suppose we have a complex problem, where we need to perform some predictions, so instead
of writing a code for it, we just need to feed the data to generic algorithms, and with the help of
these algorithms, machine builds the logic as per the data and predict the output. Machine
learning has changed our way of thinking about the problem. The below block diagram
explains the working of Machine Learning algorithm:-

Training

Building

Machine learning —_ Logical — Output

ain ML
gorithm

data Algorithm Kidda
Learn from 1
data New data
Training Data Re-train

Algorithm

False

Result
Process

Trained ML Cainer
Output

Algorithm Result

Input Data

Machine Learning Algorithm Processing Flow Chart

Features Of Machine Learning-

Machine learning uses data to detect various patterns in a given dataset.
It can learn from past data and improve automatically.

It is a data-driven technology.

El

Machine learning is much similar to data mining as it also deals with the huge amount of the
data.

Need Of Machine Leaning-

The need for machine learning is increasing day by day. The reason behind the need for
machine learning is that it is capable of doing tasks that are too complex for a person to
implement directly. As a human, we have some limitations as we cannot access the huge

amount of data manually, so for this, we need some computer systems and here comes the
machine learning to make things easy for us.

® We can train machine learning algorithms by providing them the huge amount of data and let
them explore the data, construct the models, and predict the required output automatically.
The performance of the machine learning algorithm depends on the amount of data, and it can
be determined by the cost function. With the help of machine learning, we can save both time
and money.

The importance of machine learning can be easily understood by its uses cases, Currently,
machine learning is used in self-driving cars, cyber fraud detection, face recognition, and
friend suggestion by Facebook, etc. Various top companies such as Netflix and Amazon have
build machine learning models that are using a vast amount of data to analyze the user interest
and recommend product accordingly.

Following are some key points which show the importance of Machine Learning:
o Rapid increment in the production of data.
© Solving complex problems, which are difficult for a human.
© Decision making in various sector including finance.
© Finding hidden patterns and extracting useful information from data.

Application Of Machine Learning-
® Machine learning is a buzzword for today's technology, and it is growing very rapidly day by
day. We are using machine learning in our daily life even without knowing it such as Google
Maps, Google assistant, Alexa, etc. Below are some most trending real-world applications of
Machine Learning:
1.Image Recognition-
® [mage recognition is one of the most common applications of machine learning. It is used to
identify objects, persons, places, digital images, etc. The popular use case of image recognition
and face detection is, Automatic friend tagging suggestion:
® Facebook provides us a feature of auto friend tagging suggestion. Whenever we upload a photo
with our Facebook friends, then we automatically get a tagging suggestion with name, and the
technology behind this is machine learning's face detection and recognition algorithm.
® |t is based on the Facebook project named "Deep Face," which is responsible for face
recognition and person identification in the picture.
2.Speech Recognition-
® While using Google, we get an option of "Search by voice," it comes under speech recognition,
and it's a popular application of machine learning.
® Speech recognition is a process of converting voice instructions into text, and it is also known
as "Speech to text", or "Computer speech recognition." At present, machine learning
algorithms are widely used by various applications of speech recognition. Google assistant,
Siri, Cortana, and Alexa are using speech recognition technology to follow the voice
instructions.
3.Traffic Prediction-
® |f we want to visit a new place, we take help of Google Maps, which shows us the correct path
with the shortest route and predicts the traffic conditions.
® |t predicts the traffic conditions such as whether traffic is cleared, slow-moving, or heavily
congested with the help of two ways:
© Real Time location of the vehicle form Google Map app and sensors.
o Average time has taken on past days at the same time.
® Everyone who is using Google Map is helping this app to make it better. It takes information
from the user and sends back to its database to improve the performance.
4.Product Recommendations-
® Machine learning is widely used by various e-commerce and entertainment companies such as
Amazon, Netflix, etc., for product recommendation to the user. Whenever we search for some
product on Amazon, then we started getting an advertisement for the same product while
internet surfing on the same browser and this is because of machine learning.
® Google understands the user interest using various machine learning algorithms and suggests
the product as per customer interest.
® As similar, when we use Netflix, we find some recommendations for entertainment series,
movies, etc., and this is also done with the help of machine learning.
5.Self Driving Cars-
® One of the most exciting applications of machine learning is self-driving cars. Machine learning
plays a significant role in self-driving cars. Tesla, the most popular car manufacturing company
is working on self-driving car. It is using unsupervised learning method to train the car models
to detect people and objects while driving.
6.Virtual Personal Assistant-
® We have various virtual personal assistants such as Google assistant, Alexa, Cortana, Siri. As
the name suggests, they help us in finding the information using our voice instruction. These
assistants can help us in various ways just by our voice instructions such as Play music, call
someone, Open an email, Scheduling an appointment, etc.
® These virtual assistants use machine learning algorithms as an important part.
® These assistant record our voice instructions, send it over the server on a cloud, and decode it
using ML algorithms and act accordingly.
7.0nline Fraud Detection-
® Machine learning is making our online transaction safe and secure by detecting fraud
transaction. Whenever we perform some online transaction, there may be various ways that a
fraudulent transaction can take place such as fake accounts, fake ids, and steal money in the
middle of a transaction. So to detect this, Feed Forward Neural network helps us by checking
whether it is a genuine transaction or a fraud transaction.
® For each genuine transaction, the output is converted into some hash values, and these values
become the input for the next round. For each genuine transaction, there is a specific pattern
which gets change for the fraud transaction hence, it detects it and makes our online
transactions more secure.
8.Stock Market Trading-
® Machine learning is widely used in stock market trading. In the stock market, there is always a
risk of up and downs in shares, so for this machine learning's long short term memory neural
network is used for the prediction of stock market trends.
9.Medical Diagnosis-
® |n medical science, machine learning is used for diseases diagnoses. With this, medical
technology is growing very fast and able to build 3D models that can predict the exact position
of lesions in the brain.
® |t helps in finding brain tumors and other brain-related diseases easily.
10.Automatic Language Translator-
® Nowadays, if we visit a new place and we are not aware of the language then it is not a problem
at all, as for this also machine learning helps us by converting the text into our known
languages. Google's GNMT (Google Neural Machine Translation) provide this feature, which is a
Neural Machine Learning that translates the text into our familiar language, and it called as
automatic translation.
® The technology behind the automatic translation is a sequence to sequence learning algorithm,
which is used with image recognition and translates the text from one language to another
language.

Classification Of Machine Learning-
® At a broad level, machine learning can be classified into three types:
1. Supervised learning.
2. Unsupervised learning.
3. Reinforcement learning.

Supervised Learning-

® Supervised learning is a type of
® The system creates a model using labeled data to understand the datasets and learn about
each data, once the training and processing are done then we test the model by providing a
sample data to check whether it is predicting the exact output or not.
® The goal of supervised learning is to map input data with the output data. The supervised
learning is based on supervision, and it is the same as when a student learns things in the
supervision of the teacher. The example of supervised learning is spam filtering
e Supervised learning can be grouped further in two categories of algorithms:
© (Classification-
© Regression-
® Popular Supervised Algorithms-
© Linear Regression-
© Support Vector Machine-
© Random Forest-
e Common Use cases Of Supervised Algorithns
1)Banking-
i) Used to predict the credit worthiness of credit card holder
2)Healthcare Center-
i)Used to Predict patient readmission rates
3)Retail Sector-
i)Used to analyze the product that a customer by together.

(J Unsupervised Learning-
® Unsupervised learning is a learning method in which a machine learns without any supervision.
The training is provided to the machine with the

The goal of unsupervised learning is to restructure the input data into new features or a group

of objects with similar patterns.

The machine tries to find
useful insights from the huge amount of data. It can be further classifieds into two categories of
algorithms:

© Clustering
o Association
¢ Popular Un-Supervised Algorithms-
© Apriori Algorithm.
© K-means Algorithm.
© Hierarchical Clustering.

¢ Common Use cases Of Un-Supervised Algorithms

1)Banking-

i) Segment Customer by behavioral characteristics by surveying prospects and customers to
develop multiple segments using clustering.

2)Healthcare Center-

i)Categorize the MRI data by normal or abnormal images.
AN Ratail Rartar-
JIC LHI WEwLWE

ijused to recommend the product to customer based on their past purchases.

(J Reinforcement Learning-

® Reinforcement learning is a feedback-based learning method, in which a learning agent gets a
reward for each right action and gets a penalty for each wrong action. The agent learns
automatically with these feedbacks and improves its performance. In reinforcement learning,
the agent interacts with the environment and explores it. The goal of an agent is to get the most
reward points, and hence, it improves its performance.
The robotic dog, which automatically learns the movement of his arms, is an example of

Reinforcement learning.
* Common Use cases Of Reinforcement Algorithms-
1)Banking-
i} Used to create next best offer model for a call center.
2)Healthcare Center-
i)Used to allocate scars medial resources to handle different types of ER cases.
3)Retail Sector-
ijused to reduce excess stock by dynamic pricing.

Machine Learning algorithm and it Types

Machine Learning

Clustering

¢ Linear ® SVD 3 oi ) )
® Polynomial * PCA &3 Continuous

Decision Tree ®K-means

Regression

Random forest

Association

analysis
* KNN ® Apriori 00 !
® Tress  FP-Growth 13 |

206 Categorical
00

¢Logestic Regression Hidden Markev
® Naive-Bayes Model
* SVM

(J AL,ML and Deep Learning-
1) Artificial Intelligence-

® Artificial intelligence is a technique which allows machines to mimic human behaviour.
2) Machine Learning -

® Machina laarning ic a aitheat af Al which 11ea ctatictical mathndc tna allawe machinac tn imnrave
IU HIG IG IE 19 U SU tL Ui FE YEH Ut SUISUN He UU WU GLUES HU 1 1 WW Hp wv
with experience.
3)Deep Learning -
® Deep learning is a particular kind of machine learning that is inspired by the functionality of our
brain cells called neurons which led to the concept of artificial neural network.

Mathematics For Data Science-

® Mathematics is the core of designing ML algorithms that can automatically learn from data and
make predictions. Therefore, it is very important to understand the Maths before going into the
deep understanding of ML algorithms.

® There is always a question in enthusiast learners that what is the need of mathematics in
machine learning? As computers can solve mathematics problems faster than humans. So, the
answer is, learning mathematics in machine learning is not about solving a maths
problem, rather understanding the application of maths in ML algorithms and their
working.

® Other below points explain the significance of maths in ML:

o Mathematics defines the concept behind the ML algorithms & helps in choosing the right
algorithm by considering accuracy, training time, the complexity of the model, number of
features.

© Computers understand data differently than humans, such as an image is seen as a 2D-3D
matrix by a computer for which mathematics is required.

o With Maths, we can correctly determine the interval & uncertainty.

© It helps in selecting correct parameter values and validation methods.

© Understanding the Bias-Variance trade-off helps us identify underfitting and overfitting
issues that are the main issues in ML models.

1) Median-
® A median is a number that is separated by the higher half of a data sample, a population or a
probability distribution, from the lower half.
® |f we have outliers in our dataset it is best choice to use Median because mean can be affected
by outliers.
® Median is nothing but middle value from our dataset when datapoints are arranged with
ascending order.
® Qutliers- Outliers is nothing but datapoints which are very different from rest of the datapoints.
® Median allows you to come with right guess while filling null values.
® When number of datapoints are even,then take mean of middle two values.
® Use cases of Median-
1) In simple Descriptive Statistics.
2) Handling Missing values from dataset.
2) Mean-
® Mean,is nothing but the average value of the given numbers or data.
¢ If our data is balanced and does not contains any outliers then we can use mean for filling
missing values from dataset.
® To calculate the mean, we need to add the total values given in a datasheet and then divide the
sum by the total number of values.

Use cases of Mean-

1) In simple descriptive statistics.

2) Handling missing values from dataset.

3) Mode-

A mode is defined as the value that has a higher frequency in a given set of values. It is the
value that appears the most number of times. Example: In the given set of data: 2,4, 5, 5,6, 7,
the mode of the data set is 5 since it has appeared in the set twice.
Use cases of Mode-

1) In simple descriptive statistics.

2) Fill missing values from categorical features.

4) Percentile-

Percentiles are used to understand and interpret data. The n th percentile of a set of data is the
value at which n percent of the data is below it. In everyday life, percentiles are used to
understand values such as test scores, health indicators, and other measurements.

For example-

Name>> | Naina |Abhimanyu|Sudarshan| Rohan | Roshan | Dipak Snehal Rishi Neha
Salary>> 8500 7000 5500 9500 10000 4000
0 th percentile 50 th Percentile 100 th Percentile

50 th percentile >> It means 50% datapoints are at left side and right side of this point.

25 th percentile >> It means 25% datapoints are at left side and 75 % datapoints are at right side
of this point.

75 th percentile >> It means 75% datapoints are at left side and 25% datapoints are at right side
of this point.

The range between 25 th to 75 th percentile is called as Interquartile range.

5) Variance-

The variance is one of the measures of dispersion which is a measure of how much the values in
the data set may differ from the average of the values.

tis an average of the squares of the deviations from the average.

To calculate the Variance, take each difference of values from mean, square it, and then average
the result.

6) Standard Deviation-

Standard deviation in data science is the measure of the dispersion/variation of a set of data
from its mean.

It measures the absolute variability of a distribution; the higher the dispersion is, the greater is
the standard deviation will be and greater will be the magnitude of the deviation of the value
from their mean.

SD is applicable in comparing sets of data that may have the same mean but a different range.
Standard Deviation is just the square root of Variance.

Hypothesis Testing-
A statistical hypothesis is an assumption made about a population parameter. This assumption
may or may not be right.
Hypothesis testing is a formal procedure used by statisticians to approve or disapprove a
statistical hypothesis.

“A fact is a simple statement that everyone believes. It is innocent, unless found guilty. A
hypothesis is a novel suggestion that no one wants to believe. It is guilty, until found
effective.”

Hypothesis testing is nothing but eliminate factor of randomness from the claim that you are
testing.
We have to evaluate two mutual exclusive statements on population using sample data.
Steps in Hypothesis Testing-

1) Make Initial Assumption{HO=Null Hypothesis)

2) Collect Data(Collecting Evidences)

3)Gather evidences to reject or accept null hypothesis.

If evidences is against null hypothesis then we have to reject null hypothesis. And that means
we are accepting Alternate hypothesis(H1).
There are basically five Method to conduct Hypothesis testing,

1) P Value Test-

The p-value is the probability of getting the observed value of the test statistic, or a value with
even greater evidence against Hy if the null hypothesis is actually true.

The p-value is the level of marginal significance within a statistical hypothesis test that
represents the probability of a particular event occurring. The p-value is used as an alternate
method to reject points to provide the smallest level of significance at which the null
hypothesis would be rejected. A smaller p-value means that there is stronger evidence to prove
an alternative hypothesis.

P-value is the area or the region or the size of test statistical value.

Example-Suppose an investor claims that their investment portfolio’s performance is nearly
identical to that of the Standard & Poor’s (S&P) 900 Index. In order to identify this, the investor
performed a two-tailed test.

Assume that the null hypothesis is true.
The P-Value is the probability of observing a sample mean that is as or more

extreme than the observed.

How to compute the P-Value for each type of test:

Step 1: Compute the test statistic 7, — X= Ho
c/n

Two-tail Right Tail Left Tail
Two-Tailed Right-Tailed Left-Tailed
P-value = P(Z < —|zlor Z > |zol) P-value = P(Z > z;) P-value = P(Z < z)
= 2P(Z

The sum ol The sum of
the area in

the tails 1s the

I'he area right
the area in of zis the

the tails is the P-value

P-value =
/

/

® The null hypothesis states that the portfolio’s returns are nearly equal to the S&P 900’s returns
over a specified period whereas the alternative hypothesis tells that the portfolio’s returns and
the S&P 900’s returns are not equivalent. If the investor conducts a one-tailed test, the
alternative hypothesis will tell that a portfolio’s returns are either less than or greater than the
S&P 900s returns.
® One normally uses the p-value is 0.05. If the investor comes to the conclusion that the p-value is
less than 0.05, there is strong evidence against the null hypothesis. Henceforth, the investor will
reject the null hypothesis and accept the alternative hypothesis.
® On the other hand, if the p-value is greater than 0.05, that indicates that there is weak evidence
against the supposition, so the investor will fail to reject the null hypothesis.
® |f the investor determines that the p-value is 0.001, there is strong evidence against the null
hypothesis, and the portfolio’s returns and the S&P 900’s returns may not be nearly the same.
® |n the above example, p-value helps the investor in determining the risk.
2) T test-
® When we have a continuous type of feature for evaluate significant value then we can use T test
over there.
3)One Sample Proportion Test-
® When we have one categorical feature from dataset to evaluate significant value then we can
use One Sample Proportion Test for hypothesis Testing,
4) CHI Square Test-
® When we have two categorical features to evaluate significant value,then we can use CHI square
test for Hypothesis Testing.
5) ANOVA Test-
® |f we have two features and among this one is numeric and another is categorical with more
than two categories in it then we have to use ANOVA Test.

Pandas-

® Pandas is defined as an open-source library that provides high-performance data manipulation
in Python. The name of Pandas is derived from the word Panel Data, which means an
Econometrics from Multidimensional data. It is used for data analysis in Python and
developed by Wes McKinney in 2008.

® Data analysis requires lots of processing, such as restructuring, cleaning or merging, etc.
There are different tools are available for fast data processing, such as Numpy, Scipy, Cython,
and Panda. But we prefer Pandas because working with Pandas is fast, simple and more
expressive than other tools.

® Pandas is built on top of the Numpy package, means Numpy is required for operating the
Pandas.

® Before Pandas, Python was capable for data preparation, but it only provided limited support
for data analysis. So, Pandas came into the picture and enhanced the capabilities of data
analysis. It can perform five significant steps required for processing and analysis of data
irrespective of the origin of the data, i.e., load, manipulate, prepare, model, and analyze.

Key Features Of Pandas-

- [FE FORE JRgy ESA RY J o JE PRR A TY TP USO Bi DERE RE £5 Ra ARs DA SER FSD ERD I Suviy EEDA I ION DR Te
® |TNas a rast and ermcient vatarrame ODJect WIth the JeTault and Customized inaexing.

® Used for reshaping and pivoting of the data sets.

® Group by data for aggregations and transformations.

® [tis used for data alignment and integration of the missing data.

® Provide the functionality of Time Series.

® Process a variety of data sets in different formats like matrix data, tabular heterogeneous, time
series.

® Handle multiple operations of the data sets such as subsetting, slicing, filtering, groupBy, re-
ordering, and re-shaping.

® [tintegrates with the other libraries such as SciPy, and scikit-learn.

® Provides fast performance, and If you want to speed it, even more, you can use the Cython.

Benefits Of Pandas-
® The benefits of pandas over using other language are as follows:
© Data Representation: It represents the data in a form that is suited for data analysis
through its DataFrame and Series.
o Clear code: The clear API of the Pandas allows you to focus on the core part of the code.
So, it provides clear and concise code for the user.

Read Data In The Form Of DataFrame-
® DataFrames store data in the form of rectangular grids by which the data can be over viewed
easily. Each row of the rectangular grid contains values of an instance, and each column of the
grid is a vector which holds data for a specific variable.
® This means that rows of a DataFrame do not need to contain, values of same data type, they can
be numeric, character, logical, etc. DataFrames for Python come with the Pandas library, and
they are defined as two-dimensional labeled data structures with potentially different types of
columns.
® Pandas DataFrame is a widely used data structure which works with a two-dimensional array
with labeled axes (rows and columns). DataFrame is defined as a standard way to store data
that has two different indexes, i.e., row index and column index. It consists of the following
properties:
© The columns can be heterogeneous types like int, bool, and so on.
© [t can be seen as a dictionary of Series structure where both the rows and columns are
indexed. It is denoted as "columns" in case of columns and "index" in case of rows.

1.Read Data by Using CSV-
® Syntax-
pandas.read_csv('<csv file path>')

In [11]: df = pd.read_csv('C:\\Users\Abhimanyu Devadhe\Desktop\\csv.csv")
In [12]: |df

Oout[12]:
Emp id Emp Name Emp Salary Emp Department

0 1 Jon 85000 HR

1 2 Mikel 95200 Finance
2 3 David 98563 Development
3 4 Kevin 45000 Testing

® We converted CSV.csv file into DataFrame.

® Until we have seen 2 methods for read csv file.

1. Using csv module,

2. Using pandas.

® To find current working Directory in Jupyter Notebook we use command - PWD

2. Read Data by using Excel-
® Syntax-
pandas.read_excel('path of excel file',sheet_name="<sheet name from Excel)

In [31]: dfi=pd.read _excel('C:\\Users\Abhimanyu Devadhe\Desktop\\excel.xlsx',sheet name='Sheet2')

In [32]: |df1

out[32]:
Id Mame Pass DOJ

01 Jon 12345 2012-12-12
1 2 Kevin 12345 2012-11-12
2 3 Martin 12345 2018-12-11

® We have successfully import excel sheet in the form of DataFrame.
® We can mention sheet name from Excel file,otherwise it will read first sheet by default.

3.Read Data From Dictionary-
® Syntax-
pandas.DataFrame(<dict_name>)

In [38]: | emp _records={'Id':[1,2,3,4], Name":['A','B','C','D"], "Location’:[ Pune’, 'Dhule’, ‘Nashik’, 'Jalna’]}
In [48]: df3=pd.DataFrame(emp records)

In [41]: |df3

Out[41]:
Id Name Location
0 A Pune
 W B Dhule
2 2 Cc Nashik
3 4 D Jalna

® We have successfully read data from Dictionary.

Creating Data Frame by Using Tuple List-
® Syntax-
pandas.DataFrame(<List Variable name>,Columns=["<col 1>'/<col 2>'/<col 3>']
In [46]: emp list=[('©1/12/2822"','ML Engg','Jon'),
('@2/11/2@821','DA’, Thomas"),
('e4/18/2021','DS"', 'Jason')]

In [47]: df5=pd.DataFrame(emp list,columns=['D0OJ’, ‘Designation’, "'Name'])

In [48]: dfs

outf48]:
DOJ Designation Name
0 0112/2022 WML Engg Jon
1 02/11/2021 DA Thomas
2 04/10/2021 Ds Jason

® We can give columns name to DataFrame as above.
® We can also covert above data in csv,excel,Dict,jason,pickle,sql, html etc-

In [49]: df5.to csv('csvl.csv')
In [53]: df5.to excel('excel2.xlsx')
In [54]: df5.te json('jason.jason')

In [56]: | di5.to diet{ dict")
out[s6]: {'DOJ': {e: 'ei/12/2022', 1: 'e2/11/2e21', 2: 'v4/10/2021'},

‘Designation’: {@: 'ML Engg', 1: 'DA', 2: 'DS'},
"Mame': {®: "Jon', 1: 'Thomas', 2: 'Jason'}}

® |ikewise we can convert data in any file format for backup/storage purpose.

Read Data from List of Dictionary-
® Syntax-
pandas.DataFrame(<List Of Dict_Name>)

In [85]: p=[{'Mame’: Abhimanyu’, 'Designation':'ML Engineer’, 'Salary’':85000},

{ "Name": "Sudarshan', 'Designation’:'DA’, 'Salary':68560},
{'Mame':"'Jon', 'Desiganation’:'DS', 'Salary':98326}]

In [86]: df6=pd.DataFrame(p)

In [87]: dfe

out[87]:
Name Designation Salary Desiganation
0 Abhimanyu ML Engineer 85000 Nah
1 Sudarshan DA 68500 NaN

2 Jon NaN 98326 DS
[] Read Data using Web Scraping-
® Syntax-
pandas.read_html{'<html link>")

In [91]: df7=pd.read html('https://en.wikipedia.org/wiki/virat Kohli")

In [99]: df7[1]

out[99]:

Competition Test QDI T201 FC
0 Matches 99 250 97 131
1 Runs scored 7962 12285 3296 10211
2 Batting average 5039 5878 5250 5150
3 100s/50s 27/28 43/84 0/30 34/36
4 Top score 2547 183 g4= 254%
5 Balls bowled 175 6541 146 643
6 Wickets 0 4 4 3
7 Bowling average — 166.25 4950 11266

® We can easily read data from html link with web scraping,

[") Read Data From Jason-
® Syntax-
pandas.read_jason('jason api link")

In [1e@]: df8=pd.read json('https://www.7timer.info/bin/astro.php?lon=113.2&1at=23.1&ac=08ur

In [101]: dfs

outf1e1]:
product init dataseries

0 astro 2022021906 {timepoint: 3, 'cloudcover': 9, seeing’: 7...
1 astro 2022021906 {timepoint: 6, 'cloudcover’: 9, seeing’ 7...
astro 2022021906 {timepoint: 9, 'cloudcover': 9, seeing’: 7...

astro 2022021906 [timepoint. 12, 'cloudcover: 9, 'seeing” 7...

ET EE

astro 2022021906 [timepoint. 15, 'cloudcover: 9, ‘seeing’: 7...

® | ikewise we can read Jason in Pandas.

O Operations On DataFrame-
® We can perform many Operations on DataFrame like Fetch columns, Add single column,Add
multiple column,Encode columns,Delete columns etc.
1.Fetch Columns and Rows from DataFrame-

® Syantax-
DF.shape
In [3]: |df
rE
Emp ld Emp Name Emp Salary Emp Department EDOJ Eloc Age Gender
0 1 Jon 85000 HR 12-12-2008 Pune MH 35 year Male
1 2 Mikel 95200 Finance 11-08-2011 Nashik, MH 25 Year ale
2 3 David 98563 Development 08-05-2020 Varanasi UP 22 Year Male
3 4 Laila 45000 Testing 27-09-2015 Delhi,Delhi 27 Year Female
4 5 Jason 69879 Development 28-09-2015 Jaipur, RJ 29 Year Male
5 6 Priety 45979 Development 29-09-2015 Kolakata WB 31Year Female
6 7 Naim BAT96 Testing 30-09-2015  Nagpur,MH 30 Year ale

In [4]: df.shape
out[4]: (7, 8)

® |n above we can observe that 7 rows and 8 columns are present DataFrame.

2.Fetch Specific column from DataFrame-
® Syntax-
DF['<Column_Name>"]

Irv [oly dEFElee]

Out[5]: Pune, MH
MNashik,MH

Varanasi,Up

Delhi,Delhi
Jaipur,RJ

Kolakata,WB
Nagpur, MH

Name: Eloc, dtype: object

GW RW NE ®

® Data type will be series data i.e 1 Dimensional array.

3.List of Columns From DataFrame-
® Syntax-
DF.Columns

In [7]: df.columns

out[7]: Index(['Emp Id', 'Emp Name', 'Emp Salary’, 'Emp Department’, 'EDOJ', ‘Eloc’,
‘Age’, 'Gender'],
dtype="object')

® Will give list of columns available in DataFrame.
4.Fetch Multiple Columns from DataFrame-
® Syntax-
DF[['Col_1%'Col_2",'Col_3"']]

In [12]: df[['Emp Id','Emp Salary','EDOJ', "Emp Department']]

out[12]:
Empld Emp Salary EDOJ Emp Department
0 1 85000 12-12-2008 HR
1 2 95200 11-08-2011 Finance
2 3 98563 08-05-2020 Development
3 4 45000 27-09-2015 Testing

® With the help of above command we can manipulate data.

5.Add new Column in DataFrame-

® Syntax-

DF[ '<Column_Name ' J= Value

® Value will allocate to whole rows from this newly created column.

In [14]: import datetime

In [15]: df['Last_ Updated’ ]=datetime.datetime.now()

In [16]: df
out[16]:
Empld Emp Name Emp Salary Emp Department EDOJ Eloc Age Gender Last_Updated
0 1 Jon 85000 HR 12-12-2008 Pune MH 35 year Male 2022-02-21 1532.08.704515
1 2 Mikel 95200 Finance 11-08-2011 Nashik MH 25 Year Male 2022-02-21 15:32:08.704515
2 3 David 98563 Development 08-05-2020 Varanasi UP 22 Year Male 2022-02-21 15:32:08.704515

® New created column always located at the end of dataframe.

6.Create new Dataframe from using selected columns of existing Dataframe-
® Syntax-
DF1=DF[[ Emp Id, 'Emp Name ; ' Emp Salary’, 'EDOJ']]

In [18]: dfl=df[['Emp Id', Emp Mame', Emp Salary’, 'EDOI"]]

In [19]: df1

out[19]:
Empld Emp Name Emp Salary EDOJ
0 1 Jon 85000 12-12-2008
1 2 Mikel 95200 11-08-2011
2 3 David 98563 08-05-2020

be A I mil EE=tatalal AT Nh And BE
® columns name should be give in list of list,otherwise it will show exception i.e Key Error.

7.Clean data using Lambda Function-
® Syntax-
dfif 'New_Column' J=df1[ 'Old_Column'’ ].apply (lambda_Function)

In [26]: dfi['Eloc_New']=dfi['Eloc'].apply(lambda x:x.split (',')[1])

<ipython-input-26-e7d8e96e1794>:1: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/s

rsus-a-copy
df1[ "Eloc_New']=df1['Eloc’].apply(lambda x:x.split (',")[1])

In [274]: [dF

out[27]:
Empld Emp Name Emp Salary EDOJ Eloc Eloc_New
0 1 Jon 85000 12-12-2008 Pune, MH MH
1 2 Mikel 95200 11-08-2011 Nashik, MH MH
2 3 David 98563 08-05-2020 Varanasi,UP Up

® You have to always mention New_Column_Name and on which column you have to perform
operation as syntax.

8.0peration on specific Column-
® [fyou to want to give salary hike to all your employees,
® Syntax-
DF[ 'New_Column_Name' J=DF[ 'Old_Column_Name' ].apply(lambda_Function)

In [35]: dfi[ ‘Updated salary']=dfi['Emp Salary'].apply(lambda x:x*1.3)

<ipython-input-35-2832cale3002>:1: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row indexer,col indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/use
rsus-a-copy
dfi[ ‘Updated _Salary']=dfi[ "Emp Salary'].apply(lambda x:x*1.3)

In [37]: df1
out[37]:
Emp ld Emp Name Emp Salary EDOJ Eloc Eloc_New Updated_Salary
0 1 Jon 85000 12-12-2008 Pune, MH MH 110500.0
1 2 Mikel 95200 11-08-2011 Nashik, MH MH 123760.0
2 3 David 98563 08-05-2020 Varanasi,UP 8] 128131.9

a 4 [RT ACAAN AT AQ AAAE Fall: Piatiag [Sa cacan nn

9.Encoding Columns-
® We can Encode columns with various techniques.
A)Using Lambda Function-

® Syntax

dfi] ' New_Column_Name' ]=df1[ 'Old_Column’ |.apply(Lambda_Function)

In [45]: dfi] Gender Encode Lambda']=df1['Gender'].apply(lambda x:1 if x=="Male' else @ )

<ipython-input-45-3bbc8dieebdc>:1: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row indexer,col indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/us

rsus-a-copy

df1i[ ‘Gender Encode Lambda']=df1['Gender'].apply(lambda x:1 if x=="Male' else @ )

In [46]: df1
outf[4e]:
Empld Emp Name Emp Salary EDOJ Eloc Gender Gender _Encode_Lambda
0 1 Jon 85000 12-12-2008 Pune, MH Male 1
1 2 Mikel 95200 11-08-2011 Nashik, MH Male 1
2 3 David 98563 08-05-2020 Varanasi, UP Male 1
3 4 Laila 45000 27-09-2015 Delhi,Delhi Female 0

® Lambda function is probably used when we want apply normally 2 conditions.

B)Using Map function-

® Syntax-
dfi] ' New_Column_Name' J=df1[ 'Old_Column'].map(Pass condition in Dictionary format)

n [49]: dfi[ Encode Gender Map']=dfi1['Gender'].map({ 'Male’:1, 'Female':8})

<ipython-input-49-745eh908d82d>:1: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row indexer,col indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user

rsus-a-copy
dfi[ ‘Encode Gender Map']=df1['Gender’].map({ 'Male’:1, Female':@})

Empld Emp Name

Emp Salary EDOJ

Eloc Gender

Encode_Gender_Map

‘n [58]: dfi
wt[58]:
0
1
2

2

1 Jon
2 Mikel
3 David

A I aila

85000 12-12-2008
95200 11-08-2011
98563 08-05-2020

AENANN 97 nO ON4E

Pune, MH
Nashik MH
Varanasi UP

Mate Malki

Male
Male

Male

Cormnala

1
1
1

® Map function is probably used when we want apply normally 4 to 5 conditions.

C)Using Normal Function-
® Normal function is probably used when we want apply multiple conditions.
In [61]:

In [62]:

In [63]:

out[e3]:

def encode(x):
if x=="Male':
return 1
else:
return @

dfi[ 'Encode Gender NorfFun']=df1['Gender'].apply(encode)

<ipython-input-62-e78db381dfal>:1: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/use¢
rsus-a-copy
df1[ "Encode Gender NorFun']=df1l['Gender'].apply(encode)

df1
Empld Emp Name Emp Salary EDOJ Eloc Gender Encode_Gender_NorFun
0 1 Jon 85000 12-12-2008 Pune, MH Male 1
1 2 Mikel 95200 11-08-2011 Nashik, MH Male 1
2 3 David 98563 08-05-2020 Varanasi UP Male 1
3 4 Laila 45000 27-09-2015 Delhi, Delhi Female 0

10.Delete Column from DataFrame-
® Syntax-
df.drop( 'Column_Name', axis=1,inplace=True )

In [66]:

dfl.drop( Encode Gender MNorfun',axis=1,inplace=True)

C:\Users\aAbhimanyu Devadhe\anaconda3\lib\site-packages\pandas\core\frame.p
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-doc
rsus-a-copy
return super().drop(

df1
Empld Emp Name Emp Salary EDOJ Eloc Gender
0 1 Jon 85000 12-12-2008 Pune MH Male
1 2 Mikel 95200 11-08-2011 Nashik, MH Male
2 3 Dawid 88563 08-05-2020 Varanasi UP Male
a 4 | mila ARDNN 27_0Q-201R Nalhi Nalhi  Famala

® Axis- Axis have 2 arguments,
i)0 =Interpreter will search given column name in rows of DataFrame.
ii)1 =Interpreter will search given column name in Column of DataFrame.
® |nplace- Inplace also have 2 Arguments.Inplace method is use for to commit changes in

DataFrame.
i) True= Will commit changes on Dataframe.
ii) False= Changes will appear but wont commit.

11.Delete Multiple Columns From DataFrame-
® Syntax-
DF.drop( columns=[ 'Col_1,' Col_2; 'Col_3'],axis=1,inplace=True)

In [7@]: dfi.drop( columns=[ 'EDOJ','Eloc', 'Gender'],axis=1,inplace=True)

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\pandas\core\frame.py:4308:
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable

rsus-a-copy
return super().drop(

In [71]: df1

out[71]:
Emp ld Emp Mame Emp Salary
0 1 Jon 85000
1 2 Mikel 95200
n 2 Mead neces

® Required proper syntax,if you mistakenly give space between column name it will thow error.

12.Split DOJ as Date,Month and Year-

In [56]: df['Date’]=df[ EDOJ'].apply(lambda x:x.split('-')[e])
In [51]: df[ 'Month']=df[ 'EDOJ"].apply(lambda x:x.split('-')[1])

In [52]: df["Year']=df[ EDO]'].apply(lambda x:x.split('-')[2])

In [53]: df
out[53]:
Empld Emp Name Emp Salary Emp Department EDOJ Eloc Age Gender Last_Updated Date Month Year
0 1 Jon 85000 HR 12-12-2008 Pune MH 35 year Male 2022-02-21 21:35:18.972650 12 12 2008
1 2 Mikel 95200 Finance 11-08-2011 Nashik, MH 25 Year Male 2022-02-21 21:35:18.972650 1" 08 20m
2 3 David 98563 Development 08-05-2020 Varanasi UP 22 Year Male 2022-02-21 21:35:18.972650 08 05 2020
3 4 Laila 45000 Testing 27-09-2015 DelhiDelhi 27 Year Female 2022-02-21 21:35:18.972650 ZF 09 2015
4 5 Jason 69879 Development 28-09-2015 JaipurRJ 28 Year Male 2022-02-21 21:35:18.972650 28 09 2015

® Above scenario is frequently used for data manipulation/Cleaning.

Data Definition Operations-
® We can perform Drop,Rename,Replace, Group by and Concatenation operations on columns.
1.Find Null Values from columns-
® We can find null values from each columns.
® Syntax-
df.isnull()

In [5]: #Find Null values from Date Frame-

In [7]: |df.isnull()|
out[7]:

show_id type title director cast country date_added release_year rating duration listed_in description
0 False False False False True False False False False False False False
1 False False False True False False False False False False False False
2 False False False False False True False False False False False False

a {1

® From above syntax we get output in True or False.

SE

[cE .

Hmm

A.

7

alii

Fala

® We can also find null values count from each columns-

® Syntax-
df.isnull().sum()

In [8]: df.isnull().sum()

out[8]: show id
type
title
directo
cast
country

date added

r

release year

ratine

a
® We get each columns null value count.

i

Fetan

i

SH

2. To find Unique values from specific columns-

® We can find unique values from any columns to filtrate data accordingly.

® Syntax-

df.[ '<Column_Name>' ].Unique()

In [13]: df[ date _added’].unique()

Out[13]: array(['September 25, 2021', "September 24, 2821', 'September 23, 2021°,

.., ‘December 6, 2018", "March 9, 2016', ‘January 11, 2020'],

dtype=object)

¢ |f we want find unique values in target feature then we can use above operation.

3. To find Data Type of Each column-
® We can find datatype of each columns,

® Syntax-
df.dtypes

In [17]: |df.dtype

out[17]: show id

type
title

5

director

cast

country
date add

er

object
object
object
object
object
object

nhiert

® |n pandas string is denoted as Object as per above output.

4.Change Data type of Existing Column-
® Syntax-

df ['<Column_Name>']=df[ '<Column_Name>' ].astype( '<Data_Type')

In [28]: df['release year']=df[ 'release year'].astype( object’)

In [21]: df.dtypes

out[21]: show id object
type object
title object
director object
cast object
country object
date added object
release year object
rating object

® We can change existing column data type as above.

5.Delete Rows which have null Values-
® Syntax-
df.dropna( inplace=True)

In [26]: df .dropna(inplace=True))|

In [38]: df.shape

ut]3e]: | (5332, 12)

® All rows are deleted which have null values from Data Frame.

6.Drop Null values from Rows or Columns with conditions-

® Syntax-

df.dropna( axis=0/1,how=any/all,subset[ ' Col_1', 'Col_2"] )

#0

Tn [44]:  df1.dropna(axis=e,how="any",subset=['Eno1'])|

out[44]:
Empld Emp Name Emp Salary Emp Department EDOJ Eloc Age Gender
0 1 Jon 85000 HR 12-12-2008 Pune MH 35 year Wale
1 2 Null 95200 Finance 11-08-2011 Nashik MH 25 Year vale
x 3 David 98563 NaN 08-05-2020 Varanasi UP NaN Male
3 4 Laila 45000 Testing 27-09-2015 Delhi Delni 27 Year Female
5 6 Priety 45979 Development 28-09-2015 Kolakata WB 31 Year Female

® 4th no record which contains null value has dropped from Data Frame.

7.Replace None Values from Dataframe-

® We can replace none values with user defined values from DataFrame-

Af vanlanaflnm mam “llaavr Raflnad Vialoas)
di.repiacelnp.ridr,<user_velineu_vdiue-~)

In [51]: import numpy as np

In [53]: dfil.replace(np.nan, Python")

Emp Department

EDOJ

Eloc Age Gender

out[53]:
Empld Emp Name Emp Salary
0 1 Jon
1 2 Null
2 3 David
3 4 Laila
& 5 Jason

85000
95200
98563
45000
69879

HR 12-12-2008

Finance 11-08-2011
Python 08-05-2020

Testing 27-09-2015 D

Development

® Will replace all NaN values with user defined value.

Python

Pune MH 35 year ale

Mashik, MH 25 Year ale
Varanasi, UP Python Male

elhi, Delhi 27 Year Female
Python 29 Year ale

8.We can use Replace encoding of Column-

® Syntax-

dfl.replace([ '<Col_Value>', '<Col_Value>'], ['<User_Defined_Value>', '<User_Defined_Value>'] )

In [71]: dfl.replace(['Male’, 'Female'],[1,2])

A Empld Emp Mame Emp Salary Emp Department EDOJ Eloc Age Gender
0 1 Jon 85000 HR 12-12-2008 Pune MH 35 year 1
1 2 Null 95200 Finance 11-08-2011 Nashik, MH 25 Year 1
2 3 David 98563 Python 08-05-2020 Varanasi UF Python 1
3 4 Laila 45000 Testing 27-09-2015 Delhi, Delhi 27 Year 2
4 5 Jason 69879 Development Python Python 29 Year 1
5 i] Prietv 45879 Nevelnnment 2809-2015  Kolakata WB 31 Year 7

® We can also use replace for Encoding Of columns-

9.Use Group By clause-
® syntax-
df.groupby( '<Column_Name>')

In [79]: dfi.groupby('Emp Department’)

Out[79]: <pandas.core.groupby.generic.DataFrameGroupBy object at ©x000002728F8DA280>

® |f you use above command will give you a Object,so we have to store object in variable.

In [88]: obj=df1.groupby( Emp Department”)

® Now we have to fetch group from 'Emp Department’,
Syntax= obj.get_group('<Department_Name>')

In [82]: obj.get group(' Development’)

out[s2]:

Emold Emon Name Emo Salarv Emp Department EDOJ

Eloc

Age Gender Gender Encode
MTEL  {EEEREJRERESTAS | AE MRTREEY CC SR LE
4 5 Jason 69879 Development Python Python 29 Year None
5 6 Priety 45879 Development 29-09-2015 Kolakata WB 31 Year None

® We can also find each department's size,

Syntax-
obj.size()
In [82]: obj.size()
Out[82]: Emp Department
Development 2
Finance 1
HR 1
Python 1
Testing 2

dtype: inte4

10.Rename Existing column-
® Syntax-

df.rename({'Old_Col_Name':'New_Col_Name' },axis=1,inplace=True)

In [35]: df.rename({'release year':'Release_Year', 'date_added':'Date_Added'},axis=1,inplace=True)
In [36]: df
out[36]:
show_id type title director cast country Date_Added Release Year rating duration
Kofi Ghanaba, United States,
Haile Qyafunmike Ghana, September TV- -
y Sk ETE Edina Gerima Ogunlano, Burkina Faso, 24 2021 Add MA LEH
Alexandra D. United Kin...
Mel Giedrove,

® We can also change multiple column names at once with passing in dictionary.

® We have also another method to rename columns syntax is-

df.columns=['New_Col1_Name'/New_Col2_Name'/New_Col3_Name']

Ih [53]:

In [54]: |df7

out[s4]:
EMP_Name EMP_Age EMP_City
0 Jon 22 Pune
1 Kevin 25 USA
2 Abhi 32 Ahmednagar

df7.columns=['EMP_Name','EMP_Age"','EMP _City']

11.Concatention Of Dataframe-

® We have 3 DataFrame and we have to concatenate these and make new DataFrame,
® Syntax-
df7=pd.concat{ [ df4,df5,df6] )

In [48]: df7=pd.concat([ df4,df5,df6])

In [49]: df7

out[49]:

Name Age City
0 Jon 22 Pune
1 Kevin 25 USA
2 Abhi 32 Ahmednagar
3 Rutul 23 Nashik
0 Jason 23 Ahmednagar
1 Nikhil ~~ 26 Pune

® We can concatenate multiple DataFrame at once.
® [f columns are not same in these dataframe it will show null values in that additional columns.
® We can ignore index,

In [58]: df7=pd.concat([ df4,df5,df6],ignore_index=True)

In [51]: |df7

out[51]:
Name Age City
0 Jon 22 Pune
1 Kevin 25 USA

2 Abhi 32 Ahmednagar

3 Rutul 23 Nashik
4 Jason 23 Ahmednagar
5 Nikhil 26 Pune

® With the help of Concatenation method we can add column and values to Dataframe.
In [56]: New Col=pd.Series([100,200,300])

nn [68]: df7=pd.concat([df7,New Col],axis=1)

inn [61] = |\df7
wt[s1]:
EMP_Name EMP_Age EMP_City 0
0 Jon 22 Pune 100.0
1 Kevin 25 USA 200.0
2 Abhi 32 Ahmednagar 300.0
3 Rutul 23 Nashik NaN

Aggregate Functions-
® We can perform all ageresate functions like Min.Max.Mean.Standard Deviation etc.
ER TR TEE ETI TN IEY ot Hit = thd hd TERE EEE TREE PR PEE ERP TE Rl I EE TE a

1.MAX function-

® Finding Maximum temperature from Max_Temp column,
Syntax-
df ['Max_Temp' ]==df ['Max_Temp' ].max()

In [12]: df['MaxTemp']==df[ 'MaxTemp'].max()

out[12]: eo False
1 False
2 False
3 False
4 False

361 False
® We got values in True and False ,if we we have to display in DataFrame format then add auery in
"dff 1"

In [13]: df[df['MaxTemp' ]==df[ 'MaxTemp'].max()]

out[13]:
MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDirSam WindDirdpm Wind§

71 20.9 358 0.0 9.4 133 SSE 57.0 NNW NW
1 rows x 22 columns
® |f we want only Evaporation at maximum temperature from MaxTemp column.
In [32]: df[df[ 'MaxTemp' J==df[ MaxTemp'].max()]

out[32]:
MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am W

7 209 358 0.0 94 13.3 SSE 57.0 NNW

1 rows x 22 columns

In [33]: df['Evaporation’][df[ 'MaxTemp' J==df[ MaxTemp'].max()]

out[33]: 71 9.4
Name: Evaporation, dtype: floate4

2.MIN function-

® Finding Minimum temperature from Min_Temp column,
Syntax-
df ['Min_Temp' ]==df ['Min_Temp' ].min()

In [15]: df[df[ 'MinTemp' |==df[ 'MinTemp'].min()]

EL MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDirS8am WindDii
0 209 24.3 0.0 34 6.3 NW 30.0 sw
1 209 269 36 4.4 97 ENE 39.0 E
2 20.9 234 36 58 33 NW 85.0 N

3 209 18.5 39.8 72 9.1 NW 54.0 WNW
4 20.9 16:1 28 58 10.6 SSE 50.0 SSE

® Min temp is 20.9%¢ so we got all rows which have 20.9*c temp.

3.Mean Function()-
® We can perform mean operation on any columns from DataFrame.

In [17]: df['MinTemp'].mean()

out[17]: 20.899999999999864

4,Standard Deviation of Columns-
® We can perform Standard Deviation operation on any columns from DataFrame.

In [19]: df[ 'MaxTemp'].std()

out[19]: 6.698515669598577

Comparison Operator-
® We can use comparison operator to fetch specific data from Dataframe.
Ex.1-Fetch specific row which have 15* C from MaxTemp column.

In [45]: df[df['MaxTemp']==15]

Out[45]:
MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDirSam WindDir3pm W
223 209 15.0 48 02 05 NW 450 NNW NW
356 20.9 15.0 0.8 45 1.7 Ss 70.0 Ss 5

Ex.2-Fetch rows from MaxTemp column which have temp above 35*C.

In [56]: df[df[ 'MaxTemp']>35]

Ty MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDirS8am WindDird
71 20.9 35.8 0.0 9.4 133 SSE 57.0 NNW

72 20.9 357 0.0 13.8 6.9 SW 50.0 E W

135 20.9 352 0.0 6.4 11.2 HE 48.0 SE f

3 rows x 22 columns

Ex.3-Fetch rows from MaxTemp column which have temp below 8*C.

In [55]: df[df[ 'MaxTemp']<8]

Out[55]:
MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDirfam WindDir3pm W

283 20.9 76 0.4 2.4 4.7 NW 50.0 NW NW

1 rows x 22 columns
Ex.4-We can give also one or more conditions to filter data using comparison operator,so we have
to
fetch rows which have temp between range 30 to 31*C from MaxTemp column.

In [63]: df[(df['MaxTemp']>3@) & (df[ 'MaxTemp']<31)]
out[63]:
MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDirfam WindDir3pm Wi
13 20.9 308 0.0 6.2 12.4 NW 44.0 WNW w
68 20.9 30.3 0.0 10.0 81 E 46.0 E N
137 20.9 302 0.0 7.8 11.2 NE 33.0 ESE NNW
361 20.8 30.7 0.0 76 12.1 NNW 76.0 SSE NW
365 20.9 302 0.0 6.0 12.6 NW 78.0 NW WNW
5 rows x 22 columns
Indexing-

® We can perform indexing operation on columns-
Ex.1-Set specific column at first index

In [67]: df.set index('RainTomorrow',inplace=True)
In [68]: df
out[68]:

MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir8am WindDir3pm

RainTomorrow

In [69]:

In [70]:

out[7@]:

Yes 209 243 0.0 34 6.3 NW 30.0 sw NW

Yes 209 269 36 44 97 ENE 32.0 E Ww

Yes 209 23.4 36 58 33 NW 85.0 N NNE

Yes 209 16.5 39.8 72 91 NW 54.0 WNW Ww

No 209 16.1 28 58 1086 SSE 50.0 SSE ESE

Ex.2-We can Reset/rollback index-
df .reset_index(inplace=True)

RainTomorrow MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir8am WindDir3pm ..

0 Yes 209 243 0.0 34 6.3 NW 30.0 Sw NW
1 Yes 20.9 26.9 36 4.4 97 ENE 39.0 E '\

2 Yes 209 234 36 58 3.3 NW 85.0 N NNE
3 Yes 20.9 18.5 39.8 7.2 9.1 NW 54.0 WNW Woo

4 Ne 209 16.4 28 56 10.6 SBE 50.0 SSE ESE

Loc and iLoc function-
® The Loc and iLoc functions in Pandas are used to slice a dataset and filter row wise data.
Ex.1-We can fetch only RainTomorrow= "Yes" from table.
In [93]: df.loc['ves']

ut[9e3]:
level_0 index WindGustDir MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindDirSam ...

RainTomorrow

Yes 0 0 NW 209 243 0.0 34 6.3 30.0 SW
Yes 1 1 ENE 209 269 36 4.4 97 39.0 E ..
Yes 2 2 NW 209 234 36 58 33 85.0 1
Yes 3 3 NW 209 155 39.8 72 941 54.0 WNW
Yes 8 8 5 209 19.6 0.0 4.0 41 48.0 Ey

Ex.2-We can slice data as we want,in below case i have to fetch only 3 rows and column from Emp
Name to Eloc.

In [102]: df5.loc[®@:2, "Emp Name':'Eloc']

out[102]:
Emp Name Emp Salary Emp Department EDOJ Eloc
0 Jon 85000 HR 12-12-2008 Pune MH
1 Null 85200 Finance 11-08-2011 Nashik MH
2 David 98563 NaN 08-05-2020 Varanasi, UP

® in Loc argument firstly we have to specify and rows and then as argument.

Ex.3-We have to fetch 0 to 2 index rows and Emp id to EDOJ columns.

In [104]: df5.iloc[@:3,8:5]

out[1e4]:
Empld Emp Name Emp Salary Emp Department EDOJ
0 1 Jon 85000 HR 12-12-2008
1 2 Null 95200 Finance 11-08-2011
2 3 David 98563 NaN 08-05-2020

In [165]: df5.loc[e:3]

out[1e5]:
Empld Emp Name Emp Salary Emp Department EDOJ Eloc Age Gender
1] 1 Jon 85000 HR 12-12-2008 Pune MH 35 year Male
1 2 Null 95200 Finance 11-08-2011 Nashik MH 25 Year Male
2 3 David 08563 NaN 08-05-2020 VaranasiUP Nah Male
3 4 Laila 45000 Testing 27-09-2015  Delhi,Delhi 27 Year Female

In [106]: df5.iloc[@:3]

out[1ee6]:
Empld Emp Name Emp Salary Emp Department EDOJ Eloc Age Gender
0 1 Jon 85000 HR 12-12-2008 Pune MH 35 year Male
1 2 Null 95200 Finance 11-08-2011 Nashik MH 25 Year Male
2 3 David 98563 NaN 08-05-2020 Varanasi, UP NaN Male

® We can find difference between Loc and iLoc from above example.
(J pata Normalization-
® Normalization is a pre-processing stage of any type of problem statement. In particular,
normalization takes an important role in the field of soft computing, cloud computing, etc. for
manipulation of data, scaling down, or scaling up the range of data before it becomes used for
further stages. There are so many normalization techniques there, namely Min-Max
normalization, Z-score normalization, and Decimal scaling normalization.

® Data transformation operations, such as normalization and aggregation, are additional data
preprocessing procedures that would contribute toward the success of the data extract process.
Data
normalization is generally considered the development of clean data. Diving deeper, however,
the meaning or goal of data normalization is twofold:
© Data normalization is the organization of data to appear similar across all records and
fields.
© |tincreases the cohesion of entry types, leading to cleansing, lead generation,
segmentation, and higher quality data.
1.Importance Of Data Normalization-

[J
ome Naito dese of portion of those irregularities can manifest from erasing

information, embedding more data, or refreshing existing data. Once those mistakes are
worked out and eliminated from the framework, further advantages can be acquired through
different jobs in the data and data examination.
® |t is for the most part through data normalization that the data inside a data set can be
designed so that it can be visualized and examined.
2.Data Normalization Advantages-
® We can have more clustered indexes.

® Index searching is often faster.
® Data modification commands are faster.
® Fewer null values and less redundant data, making your data more compact.
® Data modification anomalies are reduced.
® Normalization is conceptually cleaner and easier to maintain and change as your needs change.
® Searching, sorting, and creating indexes is faster, since tables are narrower, and more rows fit
on a data page.
3.Normalization Techniques at a Glance
Four common normalization techniques may be useful:
1. scaling to a range
clipping
log scaling
z-score
The following charts show the effect of each normalization technique on the distribution of the
raw feature (price) on the left. The charts are based on the data set from 1985 Ward's
Automotive Yearbook that is part of the UCI Machine Learning Repository under Automobile

oe > WM
Data Set.

Normalization Techniques
i

I
Price (raw feature) ~ Scaling to a range Clipping ) Log scaling ~ Z-score

2 » a
5
! -l Lo 1a 4 bd | Lal,

Figure - Summary of normalization techniques.

Ho ld | 1

1.Scaling to a range/Min-Max Normalization
® Scaling means converting values of features from their natural range into floating-point feature
values (for example, 100 to 900) into a standard range—usually 0 and 1 (or sometimes -1 to +1}.
Use the following simple formula to scale to a range:

X'=(x-xmin)/(xmax-xmin)

Scaling to a range is a good choice when both of the following conditions are met:

1. You know the approximate upper and lower bounds on your data with few or no outliers.

2. Your data is approximately uniformly distributed across that range.

® Agood example is age. Most age values falls between 0 and 90, and every part of the range has a
substantial number of people.

® |n contrast, you would not use scaling on income, because only a few people have very high
incomes. The upper bound of the linear scale for income would be very high, and most people
would be squeezed into a small part of the scale.

® Example-

In [31]: dfl=df[['ApplicantIncome’,'CoapplicantIncome’, 'LoanAmount’, Loan Amount Term']]

In [32]: |dfl

cael Applicantincome Coapplicantincome LoanAmount Loan_Amount_Term
0 4950 0.0 125 360
1 2882 1843.0 123 480
2 3000 3416.0 56 180

® Import class MinMaxscaler from sklearn library.

In [37]: from sklearn.preprocessing import MinMaxScaler
import pandas as pd

In [38]: obj=MinMaxScaler()

In [ 1: dfi[['ApplicantIncome’,'CoapplicantIncome’, 'LoanAmount’, Loan Amount Term']]=obj.fit transform
(df1[['ApplicantIncome’, 'CoapplicantIncome’, 'LoanAmount"’,'Loan_Amount_Term']])

In [35] ¢ [df

Out[35]:
Applicantincome Coapplicantincome LoanAmount Loan_Amount_Term

0 0.059369 0.000000 0.186277 0.729730

1 0.033791 0.054467 0.182893 1.000000

2 0.035250 0.100955 0.079526 0.324324
2.Feature Clipping

For example, you could clip all
temperature values above 40 to be exactly 40.
® You may apply feature clipping before or after other normalizations.

Formula: Set min/max values to avoid outliers.

Same feature, capped to a max of 4.0

outliers are
now 4.0

03 03
02 - 02 /
outliers
01
Lo 1 A
iy 1
00 10 00 ar a
-10 0 10 20 30 40 50 60 -1 0 1 2 3 4 5

roomsPerPerson

roomsPerPerson

Figure- Comparing a raw distribution and its clipped version.
® Another simple clipping strategy is to clip by z-score to +-No (for example, limit to +-30). Note
that ois the standard deviation.

3.Log Scaling
® |Log scaling computes the log of your values to compress a wide range to a narrow range.

x'=log(x)

n the chart below, most movies have very few ratings (the data in the tail),
while a few have lots of ratings {the data in the head). Log scaling changes the distribution,
helping to improve linear model performance.

Ratings per movie Log ratings per movie

1400 50

1200
200

1000

150
B00
600
100

400

200 8

fafsineand — 0
1000 1500 2000 2500 3000 3500 [1

| I il | | | li.

4 5 6 7 8 9

Figure- Comparing a raw distribution to its log.

4.Z-Score

You would use z-score to ensure your feature distributions have mean =0 and std = 1.
It’s useful when there are a few outliers, but not so extreme that you need clipping.
® The formula for calculating the z-score of a point, x, is as follows:

x'=(x-p)/o
® Note: uis the mean and ois the standard deviation.
price (raw feature) normalized (z-score)

5 Fi

15

10 10

wv

wm leadd | «2

0
5000 10000 15000 20000 25000 30000 35000 40000 £5000 -2 -1 4 1

md |,
2 i 4

Figure- Comparing a raw distribution to its z-score distribution.

® Notice that z-score squeezes raw values that have a range of ~40000 down into a range from
roughly -1 to +4.

® Suppose you're not sure whether the outliers truly are extreme. In this case, start with z-score
unless you have feature values that you don't want the model to learn; for example, the values
are the result of measurement error or a quirk.

® Example-

In [121]: df

Out[121]:
Applicantincome Coapplicantincome LoanAmount Loan_Amount_Term Credit History Property_Area
0 -0.037697 -0.5998486 -0.219389 0.272293 0.408868 Urban
q -0.428032 0.103374 -0.246028 2.097087 0.408868 Semiurban

2 -0.405760 0.703571 -1.138429 -2.464897 0.408868 Semiurban
3

4

0.859432

-0.531656

-0.598846 -0.392541
0.322390 -0.072875

® import scale class from sklearn

In [122]: from sklearn.preprocessing import scale

In [123]: df[col]l=scale(df[col])

0.272293 0.408368 Urban
0.272293 0.408868 Urban

In [124]: df
Outf1247]:
Applicantincome Coapplicantincome LoanAmount Loan_Amount_Term Credit_History Property_Area
1] -0.037697 -0.599846 -0.219389 0.272293 0.408868 Urban
1 -0.428032 0.103374 -0.246028 2.097087 0.408868 Semiurban
2 -0.405760 0.703571 -1.138429 -2.464897 0.408868 Semiurban
3 0.859432 -0.5995846 -0.392541 0.272293 0.408868 Urban
4 -0.531656 0.322390 -0.072875 0.272293 0.408868 Urban
Summary -
Host aktion Formula When to Use

Technique

Linear Scaling

Lj

= (8 - Bmin) (Bmax i Trnin)

When the feature is more-or-less uniformly distributed across
a fixed range.

Clipping if x > max, then x’ = max. if x < min, then x’ = When the feature contains some extreme outliers.
min
Log Scaling x = log(x) When the feature conforms to the power law.
Z-score X=(x-p)/o When the feature distribution does not contain extreme
outliers.
Merge Of Columns-

® Merging two columns in Pandas can be a tedious task if you don’t know the Pandas merging
concept. You can easily merge two different data frames easily. But on two or more columns on
the same data frame is of a different concept

Ex.1-Inner join

In [30]: pd.merge(P Details,] Details,on="Emp Id")

out[30]:
Name Age City Emp_id Dept Salary Client
0 Jon 22 Pune 1 HR 58697 HDFC
1 Kevin 25 USA 2 Dev 98000 SBI
2 Abhi 32 Ahmednagar 3 Testing 69876 CITI
3 Rutul 23 Nashik 4 ACC 79868 ICICI

® By-default merge function joins two tables with Inner Join.
Ex.2-Right merge with Matching records-

In [36]: pd.merge(P Details,J Details,on="Emp Id',how='right")

Name Age City Emp_Id uiD Dept Salary Client
0 Jon 22 Pune 1 595489 HR 58897 HDFC
1 Kevin 25 USA 2 154415 Dev 98000 SBI
2 Abhi 32 Ahmednagar 3 498494 Testing 69876 CITI
3 Rutul 23 Nashik 4 489491 ACC 79868 ICICI

Ex.3-Full outer Join-

In [44]: pd.merge(P_Details,] Details,on="Emp Id", how="cuter")

out[44]:
Name Age City Emp_ld uiD Dept Salary Client
0 Jon 22 Pune 1 595489 HR 586970 HDFC
1 Kevin 25 Usa 2 154415 Dev 98000.0 SBI
2 Abhi 32 Ahmednagar 3 498494 Testing 698760 CITI
3 Rutul 23 Nashik 4 489491 ACC 798680 ICICI
4 Neha 28 us 5 48948 NaN NaN ~~ NaN

® We got all records from both of DataFrame and we can also specify from which DataFrame data
has reflected.

In [45]: pd.merge(P Details,] Details,on="Emp Id',how='outer’,indicator=True)

Out[45]:

Name Age City Emp_Id UID Dept Salary Client _merge
0 Jon 22 Pune 1 595489 HR 58697.0 HDFC both
1 Kevin 25 USA 2 154415 Dev 98000.0 SBI both
2 Abhi 32 Ahmednagar 3 498484 Testing B9876.0 CITI both
3 Rutul 23 Nashik 4 489481 ACC 79865.0 ICICI both
4 Neha 26 us 5 438948 NaN NaN NaN left_only

Seaborn-

® |n the world of Analytics, the best way to get insights is by visualizing the data. Data can be
visualized by representing it as plots which is easy to understand, explore and grasp. Such data
helps in drawing the attention of key elements.

® To analyse a set of data using Python, we make use of Matplotlib, a widely implemented 2D
plotting library. Likewise, Seaborn is a visualization library in Python. It is built on top of
Matplotlib.
Important Features Of Seaborn-

® Seaborn is built on top of Python’s core visualization library Matplotlib. It is meant to serve as a
complement, and not a replacement. However, Seaborn comes with some very important

fnnbiivan | Abin ran a frie af bhava hava Tha fandiivan hala fn
eo O&O UI A W NH

1EdLUTES. LEL US SEC a 1Ew UI LNelm Nere. ne ieawures newp in =
Built in themes for styling matplotlib graphics.
Visualizing univariate and bivariate data.
Fitting in and visualizing linear regression models.
Plotting statistical time series data.
Seaborn works well with NumPy and Pandas data structures.
It comes with built in themes for styling Matplotlib graphics.
In most cases, you will still use Matplotlib for simple plotting. The knowledge of Matplotlib is
recommended to tweak Seaborn’s default plots.
Importing Datasets-
Seaborn comes with a few important datasets in the library. When Seaborn is installed, the
datasets download automatically.
You can use any of these datasets for your learning. With the help of the following function you
can load the required dataset.

sns.load_dataset( )
To view all the available data sets in the Seaborn library, you can use the following command
with the get_dataset_names(} function as shown below -

sns.get_dataset_names ()

Plotting Univariate Distribution-

Distribution of data is the foremost thing that we need to understand while analysing the data.
Here, we will see how seaborn helps us in understanding the univariate distribution of the data.
Function distplot() provides the most convenient way to take a quick look at univariate
distribution. This function will plot a histogram that fits the kernel density estimation of the
data.

Syntax- seaborn.distplot()

Parameters-

Sr.No. Parameter & Description

1

data

Series, 1d array or a list

bins

Specification of hist bins

hist

bool

kde

bool

Fv.
- LA-
In [43]: df2=[9@,75,60,45,36,15,45.96,56]
In [44]: bins=[]
for 1 in range(0,101,18):
bins.append(i)
In [45]: sns.displot(df2,bins=bins,kde=True)

out[45]: <seaborn.axisgrid.FacetGrid at exifdi5484310>

1.Histogram-

® Histograms represent the data distribution by forming bins along the range of the data and
then drawing bars to show the number of observations that fall in each bin.

® Seaborn comes with some datasets and we have used few datasets in our previous chapters.
We have learnt how to load the dataset and how to lookup the list of available datasets.

® [Ex-

In [50]: import seaborn as sns
In [51]: from matplotlib import pyplot as plt
In [52]: df=sns.load dataset('iris')

In [56]: sns.distplot(df[ petal length'],kde=False)

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\seaborn\distributions.py:2557: Futurel

function and will be removed in a future version. Please adapt your code to use either “disp.

similar flexibility) or “histplot™ (an axes-level function for histograms).
warnings.warn{msg, FutureWarning)

Out[56]: <Axessubplot:xlabel="petal length'>

50
40
30

10

1 4
petal_length

® Here, kde flag is set to False. As a result, the representation of the kernel estimation plot will be
removed and only histogram is plotted.

2.Kernel Density Estimates-
® Kernel Density Estimation (KDE) is a way to estimate the probability density function of a
continuous random variable. It is used for non-parametric analysis.
® Setting the hist flag to False in distplot will yield the kernel density estimation plot.

In [81]: sns.distplot(df[ petal length'],hist=False)

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\seaborn\distributions.py:25¢

function and will be removed in a future version. Please adapt your code to use eit

similar flexibility) or “kdeplot™ (an axes-level function for kernel density plots)
warnings.warn(msg, FutureWarning)

<AxesSubplot:xlabel="petal length’, ylabel="Density'>

0.25 1

0.20 1

0.05 1

petal_length

3.Fitting Parametric Distribution-
® distplot() is used to visualize the parametric distribution of a dataset.

In [82]: sns.distplot(df['petal length'])
plt.show()

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\seaborn\distributions.py:2557: Futur

function and will be removed in a future version. Please adapt your code to use either "dis

similar flexibility) or “histplot™ (an axes-level function for histograms).
warnings.warn(msg, FutureWarning)

025

0.05
0.00 T T T T 7
0 2 4 6 8

petal_length

Plotting Bivariate Distribution-

® Bivariate Distribution is used to determine the relation between two variables. This mainly
deals with relationship between two variables and how one variable is behaving with respect to
the other.

® The best way to analyze Bivariate Distribution in seaborn is by using the jointplot() function.

® Jointplot creates a multi-panel figure that projects the bivariate relationship between two
variables and also the univariate distribution of each variable on separate axes.

1.Scatter Plot-

® Scatter plot is the most convenient way to visualize the distribution where each observation is
represented in two-dimensional plot via x and y axis.

In [85]: sns.jointplot(x="petal length’,y='petal width',data=df)

out[85]: <seaborn.axisgrid.JlointGrid at exi1fdi169d76de>

25 + . "»
.
ae eee [ ]
(Xd L
orem 0 °
20 ave LI
-ae LJ
ae "aes »
Ll LJ
se .
£15 * ae me
j=} * eee .
= *  emens
= wees
® . -
10 see wo
L]
0.5 1 .
come
ses
® wes »
.
0.0 1 T T 1 r r r
1 2 3 4 5 6 7
petal_length

® The above figure shows the relationship between the petal_length and petal_width in the Iris
data. A trend in the plot says that positive correlation exists between the variables under study.
® Joint plot is used for checking Distribution.

2.Hexbin Plot-
® The above figure shows the relationship between the petal_length and petal_width in the Iris
data. A trend in the plot says that positive correlation exists between the variables under study.
® An addition parameter called ‘kind’ and value ‘hex’ plots the hexbin plot.

In [86]: sns.jointplot(x="petal length’,y='petal width',data=df,kind="hex")
plt.show()

| —.

25

—
wn

petal width

—
o

petal length

Visualizing Pairwise Relationship-

® Datasets under real-time study contain many variables. In such cases, the relation between
each and every variable should be analyzed. Plotting Bivariate Distribution for (n,2)
combinations will be a very complex and time taking process.

® To plot multiple pairwise bivariate distributions in a dataset, you can use the pairplot()
function. This shows the relationship for (n,2) combination of variable in a DataFrame as a
matrix of plots and the diagonal plots are the univariate plots.

® Parameters-

Sr.No. Parameter & Description
1 data

Dataframe
2 hue

Variable in data to map plot aspects to different colors.

palette

Set of colors for mapping the hue variable

4 kind

Kind of plot for the non-identity relationships. {'scatter’, ‘reg’}
diag_kind
Kind of plot for the diagonal subplots. {'hist’, ‘kde}
® Except data, all other parameters are optional. There are few other parameters which pairplot

can accept. The above mentioned are often used params.
® [Ex

In [91]: sns.pairplot(df,hue="species’,diag_kind="kde",kind="scatter"',palette="husl")
plt.show()

.
we
*
oF, 0.
= eo {H
g os
2 Oo
jt} “ oy
[-
2 OH
*
 .
45 ® °
a? *
4.0 1 |
LR
wn ae oe oe
5 35 © oe ® L
b=] se * = @
= ny = 3 on & 2 oe
T 30 9 Fe wer XS a
ee ee LR J Saree o
% - % Ll LJ - . LJ : iy LJ
25 $+ oe Lee eee
LI os * : . 2 ®e
01 = EE EE EEEE——.
7 . F 1 . ® setosa
o yr H f os ® versicolor
6 o & &: 1 of pie ® virginica
5 | @
£5 AB .s ] ES
§ : Ha pe .
v4 . -, 1
= a8 LIOR [ .
B31 El g .

>

25 oot
uf
on 8
20 © : = [
nme ® E ee ecoe
1 Ls LJ v L . . H “w H % Ll
2 15 o¥® nse A oF - |
2 * gs oe . ae |
% 10 A - care ee 1 |
° a | :
05 o » we 1 4
a 4 © -n
(SlE8>Piad & @ Cane
Ed [J L LL .
0.0 + T T r r T T T T , T T T
4 6 8 2 3 4 5 2 4 6 8 0 1 2 3
sepal_length sepal_width petal_length petal_width

® We can observe the variations in each plot. The plots are in matrix format where the row name
represents x axis and column name represents the y axis.

Plotting Categorical Data-
® Hexabin plots and kde plots which are used to analyze the continuous variables under study.
These plots are not suitable when the variable under study is categorical.
® When one or both the variables under study are categorical, we use plots like striplot(),
swarmplot(), etc,. Seaborn provides interface to do so.
1.Stripplot( )-
® stripplot() is used when one of the variable under study is categorical. It represents the data in
sorted order along any one of the axis.

In [94]: sns.stripplot(x='species',y="petal length',data=df)
plt.show()

74

.
re
s
6 4
| .
L

5 |
FE = =
2]
=
=
54 xd
I
| * H
T° CL]
3 s
2 .
1 .*
T T T
setosa versicolor virginica
species

® |n the above plot, we can clearly see the difference of petal_length in each species. But, the
major problem with the above scatter plot is that the points on the scatter plot are overlapped.
We use the Jitter’ parameter to handle this kind of scenario.

® Jitter adds some random noise to the data. This parameter will adjust the positions along the
categorical axis.

In [98]: sns.stripplot(x="'species’,y="petal length',data=df,jitter=True)
plt.show()

T

wn on

»

9 >
-

petal_length
I'S

w
».

]
.
.

T T T
etosa versicolor virginica
species

2.swarmplot( )-
¢ Another option which can be used as an alternate to ‘Jitter’ is function swarmplot(). This
function positions each point of scatter plot on the categorical axis and thereby avoids
overlapping points —

In [108]: sns.swarmplot(x='species',y="petal length',data=df)

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\seaborn\categorical.py:1296:
placed; you may want to decrease the size of the markers or use stripplot.
warnings.warn(msg, UserWarning)

Out[18@]: <Axessubplot:xlabel="species', ylabel="petal length’>

1 S
~

6

5 4
FF.
f=) L J
o 4
5 Ie
& 4 pe

21 se

1 4 3

setosa versicolor virginica
species

Distribution Of Observations-

® |n categorical scatter plots which we dealt in the previous chapter, the approach becomes
limited in the information it can provide about the distribution of values within each category.
Now, going further, let us see what can facilitate us with performing comparison with in
categories.

1. Box Plots-

® Boxplot is a convenient way to visualize the distribution of data through their quartiles.

® Box plot is mainly used for to identify outliers.

® Box plots usually have vertical lines extending from the boxes which are termed as whiskers.
These whiskers indicate variability outside the upper and lower quartiles, hence Box Plots are

also termed as box-and-whisker plot and box-and-whisker diagram. Any Outliers in the data are
plotted as individual points.

In [183]: sns.boxplot(x='species’,y="petal length',data=df)

<hxesSubplot:xlabel="species’, ylabel="petal length’>

|

6

5

petal_length
n

2
1
T 7 T
setosa versicolor virginica
species

® The dots on the plot indicates the outlier.

2Violin Plots-

® Violin Plots are a combination of the box plot with the kernel density estimates. So, these plots
are easier to analyze and understand the distribution of the data.
® |et us use tips dataset called to learn more into violin plots. This dataset contains the
information related to the tips given by the customers in a restaurant.
In [8]: sns.violinplot(x="day’',y='total bill',data=df)

Out[8]: <AxesSubplot:xlabel='day', ylabel="total bill's»

80 4

bid

day

50

total _bill
8g 8

[%)
o

<3

[=]

® The quartile and whisker values from the boxplot are shown inside the violin. As the violin plot
uses KDE, the wider portion of violin indicates the higher density and narrow region represents
relatively lower density. The Inter-Quartile range in boxplot and higher density portion in kde
fallin the same region of each category of violin plot.

® The above plot shows the distribution of total_bill on four days of the week. But, in addition to
that, ifwe want to see how the distribution behaves with respect to sex, lets explore it in below
example.

In [12]: sns.violinplot(x='day',y='total bill", hue="sex', data=df)

Out[18]: <AxesSubplot:xlabel='day', ylabel="total bill'>

sex
EN Male
EE Female

8 & 8 8

total_bill

(a)
oo
i

=
o

[=]

day

® Now we can clearly see the spending behavior between male and female. We can easily say
that, men make more bill than women by looking at the plot.

® And, if the hue variable has only two classes, we can beautify the plot by splitting each violin
into two instead of two violins on a given day. Either parts of the violin refer to each class in the
hue variable.

Statistical Estimation-

a ln omanack afbha flbiiabiane aia daal aslbh anklnatinne af tha vhala dickvilidkian af ilha dada Dod
* AN IOSL OI LUNE SILUdUOInS, we acdl WILT €SLHTIAdLUOINS OI LUNE WHOLE UISUITRDULION Ol LUNE Udld. DUL
when it comes to central tendency estimation, we need a specific way to summarize the
distribution. Mean and median are the very often used techniques to estimate the central
tendency of the distribution.

© |n all the plots that we learnt in the above section, we made the visualization of the whole
distribution. Now, let us discuss regarding the plots with which we can estimate the central
tendency of the distribution.

1. Bar Plots-

® |n all the plots that we learnt in the above section, we made the visualization of the whole
distribution. Now, let us discuss regarding the plots with which we can estimate the central
tendency of the distribution.

® Bar plot represents the estimate of central tendency. Let us use the ‘titanic’ dataset to learn bar
plots.

In [13]: sns.barplot(x='sex',y="survived',hue='class',data=df3)

Out[13]: <AxesSubplot:xlahel='sex", ylahel='survived'>

10 dass
EEE First
EN Second
EEE Third

o
[=]

survived
=]
o

bd
re

[=]
~N

0.0
male female

sex
® Inthe above example, we can see that the average number of survivals of male and female in
each class. From the plot we can understand that more number of females survived than males.
In both males and females more number of survivals are from first class.
® Aspecial case in barplot is to show the no of observations in each category rather than
computing a statistic for a second variable. For this, we use countplotf).

In [14]: sns.countplot(x='class',data=df3)

Out[14]: <AxesSubplot:xlabel='class', ylabel='count'>

500

400

300

count

First Second Third

ass
as

® Plot says that, the number of passengers in the third class are higher than first and second class.

2.Point Plots-

® Point plots serve same as bar plots but in a different style. Rather than the full bar, the value of
the estimate is represented by the point at a certain height on the other axis.

In [16]: sns.pointplot(x="'sex',y='survived',hue="'class’',data=df3)

Out[16]: <AxesSubplot:xlabel='sex', ylabel="survived'>

101 dass
@® First
® Second
08 ® Third
T 061
=
7
041
021

T
male female

Plotting Wide Form Data-

® [tis always preferable to use ‘long-from’ or ‘tidy’ datasets. But at times when we are left with no
option rather than to use a ‘wide-form’ dataset, same functions can also be applied to “wide-
form” data in a variety of formats, including Pandas Data Frames or two-dimensional NumPy
arrays. These objects should be passed directly to the data parameter the x and y variables
must be specified as strings.

In [19]: sns.boxplot(data=dfl,orient="h")

Out[19]: <AxesSubplot:>

sepal_length [11 |
sepal_width 1 L] A

Multi-Panel Categorical Data-
® Categorical data can we visualized using two plots, you can either use the functions pointplot(),
or the higher-level function factorplot().
1.FactorPlot()-
® Factorplot draws a categorical plot on a FacetGrid. Using ‘kind’ parameter we can choose the
plot like boxplot, violinplot, barplot and stripplot. FacetGrid uses pointplot by default.

In [23]: sns.factorplot(x="time',y='pulse’,hue="kind',data=df4)

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\seaborn\categorical.py:3714: UserWar

130

120

# kind
Z 10 ® rest
® walking
® mnning
100
90
T T T
1min 15 min 30 min

time

Linear Relationships-

® Most of the times, we use datasets that contain multiple quantitative variables, and the goal of
an analysis is often to relate those variables to each other. This can be done through the
regression lines.

® While building the regression models, we often check for multicollinearity, where we had to see
the correlation between all the combinations of continuous variables and will take necessary
action to remove multicollinearity if exists. In such cases, the following techniques helps.
Functions to Draw Linear Regression Models-

® There are two main functions in Seaborn to visualize a linear relationship determined through
regression. These functions are regplot() and Implot().

regplot vs Implot

regplot Implot

accepts the x and y variables in a variety of formats ~~ has data as a required parameter and the x

including simple numpy arrays, pandas Series and y variables must be specified as
objects, or as references to variables in a pandas strings. This data format is called “long-
DataFrame form” data

Ev 1_Dlnttino tha raonlat and than Imnlat with tha cama Aata in thic avamnla
E=fek™I LULL 5 ui 1ICHM VL ali unica Ulipive VVILIL LLIT Dall uaa ni Lio TAalnpc

In [28]: sns.regplot(x='total bill',y="tip',data=df)
sns.lmplot(x = “total bill", y = "tip", data = df)

Out[28]: <seaborn.axisgrid.FacetGrid at @x19513c46c40>

10 1 L

tip

10 20 30 a0 50
total _bill
10 1
°
BA
°
.
* .
1 e @
®
& ul
se $e LY
bet °
rg *
 ] ®
"we
*
21
bes & oo ®
T T T T T
10 20 30 40 50

total_bill

® You can see the difference in the size between two plots.

Matplotlib-

Matplotlib is one of the most popular Python packages used for data visualization. It is a cross-
platform library for making 2D plots from data in arrays. Matplotlib is written in Python and
makes use of NumPy, the numerical mathematics extension of Python. It provides an object-
oriented API that helps in embedding plots in applications using Python GUI toolkits such as
PyQt, WxPythonotTkinter. It can be used in Python and IPython shells, Jupyter notebook and

weh annlication servers alsn
RT eT LCA)

® Matplotlib has a procedural interface named the Pylab, which is designed to resemble MATLAB,
a proprietary programming language developed by MathWorks. Matplotlib along with NumPy
can be considered as the open source equivalent of MATLAB.

® Matplotlib was originally written by John D. Hunter in 2003. The current stable version is 2.2.0
released in January 2018.

1. Bar Plot-

® Abar chart or bar graph is a chart or graph that presents categorical data with rectangular bars
with heights or lengths proportional to the values that they represent. The bars can be plotted
vertically or horizontally.

® Anbar graph shows comparisons among discrete categories. One axis of the chart shows the
specific categories being compared, and the other axis represents a measured value.

Ex.1-

In [2]: from matplotlib import pyplot as plt

In [3]: plt.bar([2,4,6,8,10,12],[22,28,36,44,52,60])

<BarContainer object of 6 artists»

60 Bi

30 A
20 1
04
2 4 6 8 10

Ex.2-We can pass list as x and y axis.

In [9]: bjp=[1,3,5,7,9,11]
congress=[5,9,1,2,9,11]

In [10]: plt.bar(bjp,congress)

Out[1@]: «<BarContainer object of 6 artists»

10

11
= AR
4 6 8 10

Ex.3-We can give lables to plot and axis.

In [19]: |data={'C':2@,'C++':28, Java’:18, 'Python':45, 'R+':40}
course=list(data.keys())
students=1list(data.values())

fig=plt.figure(figsize=(8,5))
#Creating Bar Graph

plt.bar(course, students)

plt.xlabel( Course Name")
plt.ylabel('students Enroll’)
plt.title( students Enrcll vs Course')|

out[19]: Text(®.5, 1.0, ‘Students Enroll Vs Course’)

Students Enroll Vs Course

40

= 30
e
=
w
4]
[=
=]

3 20
wi

10

0

Java Python R+
Course Name
2.Histogram-

® Ahistogram is an accurate representation of the distribution of numerical data. It is an estimate
of the probability distribution of a continuous variable. It is a kind of bar graph.
® To construct a histogram, follow these steps —
© Bin the range of values.
© Divide the entire range of values into a series of intervals.
© Count how many values fall into each interval.
The bins are usually specified as consecutive, non-overlapping intervals of a variable.

The matplotlib.pyplot.hist() function plots a histogram. It computes and draws the histogram of
X.

® Parameters-

X array or sequence of arrays
bins integer or sequence or ‘auto’, optional
optional parameters
range The lower and upper range of the bins.

density If True, the first element of the return tuple will be the counts normalized to form a
probability density

cumulative If True, then a histogram is computed where each bin gives the counts in that bin plus
all bins for smaller values.

histtype The type of histogram to draw. Default is ‘bar’
= ‘bar’ is a traditional bar-type histogram. If multiple data are given the bars are
arranged side by side.

= ‘barstacked’ is a bar-type histogram where multiple data are stacked on top
of each other.

= ‘step’ generates a lineplot that is by default unfilled.

= ‘stepfilled’ generates a lineplot that is by default filled.

Example-

In [26]: class age=[16,18,17,26,25,25,15,14,12,9]
bins=[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]

In [27]: plt.hist(class age,bins=bins)
OUt[27]: (array([B.; @:; Os; Big diy Big Lag 2a; 25 Tog Oss Bey 2s; 1:1)

array([ @¢, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]),
<BarContainer object of 14 artists»)

200

0.00 -

3.PieChart-
® A Pie Chart can only display one series of data. Pie charts show the size of items (called wedge)
in one data series, proportional to the sum of the items. The data points in a pie chart are
shown as a percentage of the whole pie.
® The pie chart looks best if the figure and axes are square, or the Axes aspect is equal.
® Parameters-

Following table lists down the parameters foe a pie chart -
X array-like. The wedge sizes.
labels list. A sequence of strings providing the labels for each wedge.

Colors A sequence of matplotlibcolorargs through which the pie chart will cycle. If None, will use
the colors in the currently active cycle.

Autopct string, used to label the wedges with their numeric value. The label will be placed inside
the wedge. The format string will be fmt%pct.

Example-

In [45]: Students Count=[25,68,78,84]
Batch _No=[ 'Python24', 'Python25', 'Python26", "Python27"]

In [47]: plt.pie(Students Count,labels=Batch No, shadow=True,explode=(9.3,0,0,0))

Python25

A =

Python26

4.Scatter Plot-

® Scatter plots are used to plot data points on horizontal and vertical axis in the attempt to show
how much one variable is affected by another. Each row in the data table is represented by a
marker the position depends on its values in the columns set on the X and Y axes. A third
variable can be set to correspond to the color or size of the markers, thus adding yet another
dimension to the plot.

® This plot is also used for Outlier detection and for Univariate analysis and Bivariate analysis.
Ex.1-

In [55]: weight=[55,65,68,72,70,72,85,75,80]
height=[162,175,160,164,166,165,167,152,172]

In [56]: plt.scatter(weight,height)

Out[56]: <matplotlib.collections.PathCollection at @x255aced2310>
175 L
L ]
170 1
L J
165 - dl
®
[ J
160 °
155
LJ]
55 80 & 70 : 80 8

Ex.2-

In [64]: plt.scatter(df['MaxTemp'],df[ Sunshine'])

Out[64]: <matplotlib.collections.PathCollection at @x255afb84520>

14

a _ & > PIA

@
* or’ <e > we .
- *e*%. E
21 Be Pe ® e ® 3]
® ™ EL)
0 Lg od’ o?er 2°
10 15 20 2 0 35

5.Line Graph-

In [111]: x=[12,15,20,25,29,35,40]
In [112]: |y=[40,35,29,25,20,15,12]

In [113]:  plt.plot(x,y)
plt.xlabel('X")
plt.ylabel('Y")
plt.title( ‘Relationship Between X and Y')

Out[113]: Text(@.5, 1.0, ‘Relationship Between X and VY")

Relationship Between X and Y

EEN

5

20

6.Heat Map-
® Heatmap is basically used for feature selection and to find co-relation between features.

Ex.1-

In [18]: df5=sns.load dataset('iris')

In [24]: sns.heatmap(data=(df5[ sepal length'],df5[ sepal width']))

Out[24]: <AxesSubplot:>

Numpy -

® NumPy stands for numeric python which is a python package for the computation and
processing of the multidimensional and single dimensional array elements.

® Travis Oliphant created NumPy package in 2005 by injecting the features of the ancestor
module Numeric into another module Numarray.

® [tis an extension module of Python which is mostly written in C. It provides various functions
which are capable of performing the numeric computations with a high speed.

® NumPy provides various powerful data structures, implementing multi-dimensional arrays and
matrices. These data structures are used for the optimal computations regarding arrays and
matrices.

® With the revolution of data science, data analysis libraries like NumPy, SciPy, Pandas, etc. have
seen a lot of growth. With a much easier syntax than other programming languages, python is
the first choice language for the data scientist.
NumPy provides a convenient and efficient way to handle the vast amount of data. NumPy is
reasonable to work with a large set of data.
® There are the following advantages of using NumPy for data analysis.
1. NumPy performs array-oriented computing.

N

>

5

It efficiently implements the multidimensional arrays.

It performs scientific computations.

Itis capable of performing Fourier Transform and reshaping the data stored in
multidimensional arrays.

NumPy provides the in-built functions for linear algebra and random number generation.

® Nowadays, NumPy in combination with SciPy and Mat-plotlib is used as the replacement to
MATLAB as Python is more complete and easier programming language than MATLAB.
® Why we are using Numpy when we have List..??

1

2
3

Its occupy less memory compare to list.
Its actually pretty fast.
It is very convenient to work with NumPy.

1.Create One Dimensional Array-

® Syntax-
np.array([1,2,3,4])
In [2]: import numpy as np
In [3]: a=np.array([1,2,3,4])
In [4]: a
Outf[4]: array([1l, 2, 3, 4])

2. Create Two Dimensional Array-

mm [#7]:

In [8]:

Out[8]:

b=np.array([(1,2, 3,4), (556575 8)1)

b

array([[1, 2, 3, 4],
[5, 6, 7, 8]])

3.Find The Dimensions Of The Array-
® Syntax=
<array>.ndim

In [18]: b

Out[18]: array([[1, 2, 3, 4],

[5 6, 7, 8]])
In [12]: | b.ndim

Qut{12]: 2

4.Find Byte size of Array-
® Syntax-
<array>.itemsize

In [26]: (Bb

out[20]: array([[1, 2, 3, 4],
[5, 6, 7, 811)

In [19]: b.itemsize

out[19]: 4

5.Find Data Type Of Array-
® Syntax-
<array>.dtype

In [22]: |b

out[22]: array([[1, 2, 3, 4],
[55 6, 7, 211)

In [21]: b.dtype

Out[21]: dtype('int32")

6.Find Shape Of Array-
® Shape of array nothing but no of columns and rows present in array.
® Size of Array will give no of elements present in array.

In [25]: |b

Out[25]: array([[1, 2, 3, 4],
[5, 6, 7, 8]])

In [24]: b.size

out[24]: 8

In [26]: b.shape

out[26]: (2, 4)

7.Reshape Of Arrays-
® When you change number of columns and rows that is called reshaping.

Th [27]: | esnp-array([(1,2,3,4,5) :(4,5:6,;7,8) 1)

In [28]: |c

Out[28]: array([[1, 2, 3, 4, 5],
[4, 5, 6, 7, 811)

In [29]: c.shape

out[29]: (2, 5)

In [36]: c.reshape(5,2)

Out[36]: array([[1, 2],
[3, 41,
[5 41,
[5, 6],
[7, 811)

8.Slicing-
® Slicing is basically extracting set of elements from your array.
® When we say 0: this actually means all the rows that will include 0 as well.

In [39]% |e
Out[39]: array([[1l, 2, 3, 4, 5],
[4, 5, 6, 7, 811)
In [50]: c[©:,0] #if we have to fetch zeroth index from each rows

Out[50]: array([1, 4])

In §5%]: (e[E 5] #if we have to fetch first index value from 2nd row

Out[51]: array([5])

In [52]: leas] #if we have to fetch only first row from array

Out[52]: array([[1, 2, 3, 4, 511)

In [54]: | c[1:2] #1f we have to fetch only second row from array

Out[54]: array([[4, 5, 6, 7, 811)

In [62]: c[@:2] #1f we have to fetch first and second row from array
Out[62]: array([[1, 2, 3, 4, 5],
[4, 5, 6, 7s 811)

9.Find Min,Max,Mean,Standard Deviation of array
® We can perform Min,Max,Mean,Standard Deviation operations on array .

In [69]: |€

Out[69]: array([[ 1, 2, 3, 4, 5],
[ 4, 5 +6, 7, 8],
[ 7; & 9; 1s, 11];
[5 6 7, 9,10]])

In [82]: c.max(),c.min(),c.sum(),c.mean(),c.std(),

out[82]: (11, 1, 127, 6.35, 2.6884010117540127)

10.Axis-
® axis =0 >>Refer to Columns
® axis =1>> Refer to Rows
In [83]:

Out[83]:

Fi [85%

Out[85]:

In [86]:

Out[86]:

array([[ 1, 2, 3, 4,
[ 4, 5, 6, 7,
L Zs. 8, 9,48,
L 5s HE, fs "9,

c.sum(axis=@)

array([17, 21, 25, 30, 34])

c.sum(axis=1)

array([15, 38, 45, 37])

11.Arithematic Operations on Arrays-
® We can perform Matrix addition, Subtraction,Multiplication and Division on array

In [97]: c+d,c-d,c*d,c/d

Out[97]: (array([[ 2, 4,
[ 8, 180,

[14, 16,

[1e, 12,
array([[©, ©,
[@, 0,
[0, 0,
[e, 0,
1,
16, 2

10],
16],
22],
20]1),

25],

64],
1217,
10011),

array([

array([

2

E

[

[

[

[ 25, E

[Tos Donny Fay Loy 2
fl. A, do Ld,
[as Tog A Tag 1
[lias Mog Wag Tap A

E]

12.Concatenate Arrays-
® We have two methods of stack i.e Vertical stack and Horizantal stack

In [1@8]: np.vstack((c,d))

out[100]: array([[

2

[RET
oe ew

-

3

-

1
4
7
5
3
4
7
5

alae Nee Ne Ne Ne Ne |
-

Lent «= REV a BN NN aL J es EV IY
-

~N OWS Od WwW
[
i=
-

2

-

In [181]: np.hstack((c,d))

Out[1@1]:

-
ES
Z

51,

81,
1],
1011)

array([

-
-

-
~
-

-

OI
G

“

-
pus]
-

-

-
00 nM
-
~Wooon w
-

13.Convert Multidimensional array to single column-

In T1@61: Ic
—— pm -

Out[1@6]: array([[ 1, 2, 3, 4, 51,
[4, 5, 6; 7; 8];
[7, 8, 9,18, 11],
[5 6 7, 9,10]])

In [1@5]: np.ravel(c)

out[105]: array([ 1, 2, 3, 4, 5, 4, 5, 6, 7, 8, 7, 8 9,18, 11, 5, 6,
7, 9, 101)

[J Exploratory Data Analysis-
re E——
® |n statistics, exploratory data analysis is an approach to analyzing data sets to summarize
their main characteristics, often with visual methods. A statistical model can be used or not, but
primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis

testing task.
® EDA in Python uses data visualization to draw meaningful patterns and insights. It also involves

the preparation of data sets for analysis by removing irregularities in the data.

® Based on the results of EDA, companies also make business decisions, which can have
repercussions later.

® [f EDA is not done properly then it can hamper the further steps in the machine learning model
building process.

® JfEDA is not done properly then it can hamper the further steps in the machine learning model

building process.
® /fdone well, it may improve the efficiency of everything we do next.
EDA contains below topics-
Data Sourcing.
Data Cleaning.

Univariate analysis.
Bivariate analysis.

LAE a

Multivariate analysis.

A)Data Sourcing-
® Data Sourcing is the process of finding and loading the data into our system. Broadly there are
two ways in which we can find data.
1. Private Data-
2. Public Data-
Private Data-
® As the name suggests, private data is given by private organizations. There are some security
and privacy concerns attached to it. This type of data is used for mainly organizations internal
analysis.
Public Data-

a This tina af Naka ia avallahla dba Avanmranma Wa can Bed this fn caciavnms and aoahelban and mo hilia
® 1THS LYyPE Ul vdld IS avdildDIE LV EveryOne. vwe carl Hu Lis in government weosiLes dru puvltc
organizations etc. Anyone can access this data, we do not need any special permissions or
approval.

® The very first step of EDA is Data Sourcing, we have seen how we can access data and load into
our system. Now, the next step is how to clean the data.

B)Data Cleaning-

® After completing the Data Sourcing, the next step in the process of EDA is Data Cleaning. It is
very important to get rid of the irregularities and clean the data after sourcing it into our system.
Irregularities are of different types of data.

Missing Values-

Incorrect Format-

Incorrect Headers-

Fal CAC

Anomalies/Outliers-
5. Standardizing Values-
i)Missing Values-
® [f there are missing values in the Dataset before doing any statistical analysis, we need to
handle those missing values.
® There are mainly three types of missing values.
1. MCAR(Missing completely at random): These values do not depend on any other features.
2. MAR(Missing at random): These values may be dependent on some other features.
3. MNAR(Missing not at random): These missing values have some reason for why they are

missing.

ii)Anomalies/Outliers-
* Outliers are the values that are far beyond the next nearest data points.
® There are two types of outliers:
1. Univariate outliers: Univariate outliers are the data points whose values lie beyond the
range of expected values based on one variable.
2. Multivariate outliers: While plotting data, some values of one variable may not lie beyond
the expected range, but when you plot the data with some other variable, these values may
lie far from the expected value.

OUTLIERS AND ANOMALIES 2

Variable 2
o

Univariate Outlier : Variable 1

Multivariate Outlier

® So, after understanding the causes of these outliers, we can handle them by dropping those
records or imputing with the values or leaving them as is, if it makes more sense.
iii)Standardizing Values-
® To perform data analysis on a set of values, we have to make sure the values in the same
column should be on the same scale. For example, if the data contains the values of the top
speed of different companies’ cars, then the whole column should be either in meters/sec scale
or miles/sec scale.

C)Univariate Analysis-
® |f we analyze data over a single variable/column from a dataset, it is known as Univariate
Analysis.
® |f the column or variable is of numerical then we’ll analyze by calculating its mean, median, std,
etc. We can get those values by using the describe function.
I
i)Categorical Unordered Univariate Analysis-
® An unordered variable is a categorical variable that has no defined order.

ii)Categorical Ordered Univariate Analysis-
® Ordered variables are those variables that have a natural rank of order.

D)Bivariate Analysis-
¢ |f we analyze data by taking two variables/columns into consideration from a dataset, it is
known as Bivariate Analysis.
i)Numeric -Numeric Analysis-
® Analyzing the two numeric variables from a dataset is known as numeric-numeric analysis. We
can analyze it in three different ways.

© Scatter Plot-

© Pair Plot-

o Correlation Matrix-It is difficult to see the relation between three numerical variables in a
single graph. In those cases, we’ll use the correlation matrix and then we use this matrix for
mapping of Heatmap.

ii)Numeric - Categorical analysis-
® Analyzing the one numeric variable and one categorical variable from a dataset is known as
numeric-categorical analysis. We analyze them mainly using mean, median, and box plots.
iii) Categorical- Categorical-
® Analyzing the two categorical dataset is known as categorical-categorical analysis

E)Multivariate Analysis-
® |f we analyze data by taking more than two variables/columns into consideration from a
dataset, it is known as Multivariate Analysis.

(J conclusions-
® This is how we’ll do Exploratory Data Analysis. Exploratory Data Analysis (EDA) helps us to look
beyond the data. The more we explore the data, the more the insights we draw from it. As a
data analyst, almost 80% of our time will be spent understanding data and solving various
business problems through EDA.

(CJ Random Oversampling and Under-sampling for Imbalanced classification-
® |mbalanced datasets are those where there is a severe skew in the class distribution, such as
1:100 or 1:1000 examples in the minority class to the majority class.
® This bias in the training dataset can influence many machine learning algorithms, leading some
to ignore the minority class entirely. This is a problem as it is typically the minority class on
which predictions are most important.
® One approach to addressing the problem of class imbalance is to randomly resample the
training dataset. The two main approaches to randomly resampling an imbalanced dataset are

® |n short we can say that,
o Random Oversampling: Randomly add duplicate examples in the minority class.
o Random Under-sampling: Randomly delete examples in the majority class.
* Random Re-Sampling -Applying re-sampling strategies to obtain a more balanced data
distribution is an effective solution to the imbalance problem.
¢ Examplel- For Oversampling .
1.Load data set-

In [3]: df=pd.read_csv('creditcard.csv")

In [4]: df
Out[4]:
Time v1 v2 V3 V4 V5 Vvé v7 vs ve
0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ..
1 00 1.191857 0266151 0166480 0448154 0060018 -0082361 -0078803 0085102 -0255425
2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 _.
3 10 -0966272 -0185226 1792993 -0863291 -0010309 1247203 0237608 0377436 -1387024
4 20 -1.158233 0.877737 1.548718 0.403034 -0407193 0.095921 0.592941 -0.270533 0817739 ..
2.Check DataFrame Shape-

In [5]: df.shape

out[5]: (284807, 31)

3.Find values counts from targeted features-

In [6]: df['Class'].value counts()

Out[6]: @ 284315
1 A497
i"

Name: Class, dtype: int64

wae

4.Split Target feature and Independent Feature-

® There are several ways for splitting,
df.ioc[:, 0:30]
Using list Comprehensions-

1. Usingiloc-

2.
3. Using Drop -
[J

Split Data as X= Independent Features and Y = Target Feature

In [18]: X=df.drop(’'Class',axis=1)

[ col for col in df.columns if col I= "Class']
df.drop(' Class")

In [11]: |X
Out[11]:
vs Ve v7 ve ve .. va20 v21 v22 v23 va4 Vas V26 var V28 Amount
0.338321 0.462388 0.239599 0.098698 0.363787 0.251412 -0.018307 0.277838 -0.110474 00866928 0.128539 -0.189115 0.133558 -0.021053 149.62
0.060018 -0.082361 -0.078803 0.085102 -0.255425 -0.069083 -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 269
0.503198 1.800499 0.791461 0.247676 -1.514654 0.524980 0.247998 0.771679 0.909412 -0689281 -0.327642 -0.138097 -0.055353 -0.059752 378.66

In [12]: [Y=Hf["Class™]

In [14]: Y.shape

out[14]: (284807,)

5.Check Value counts from Target Feature-

In [15]: Y.value counts()

Out[15]: © 284315
al 492
Name: Class, dtype: int64

6.Sampling -

® As we can observed that from Values counts ,data is imbalance so we have to use do Over-

sampling.

® So install module package imblearn in your machine, and import class- RandomOverSampler
and create object of class and give customize percentage of matching/duplication of data.

In [21]: from imblearn.over sampling import RandomOverSampler

In [ ]: obj=RandomOverSampler(©.85)

7. Fit Samples-
® Fit samples to X_new and Y_Nnew
® After fitting of samples, we can see that Target features have near about same datapoints for 0
and 1.

In [32]: X_new,Y_new=ob7j.fit resample(0.85)

In [37]: X.shape

out[37]: (284807, 30)

In [35]: Y new.value counts()

Out[35]: @ 284315
1 213236
Name: Class, dtype: inte4

Example 2- For Under-Sampling-
8. Import Class NearMiss-
® import class NearMiss,and create object .

In [38]: from imblearn.under_ sampling import NearMiss

In [39]: objl=NearMiss(@.85)

C:\Users\Abhimanyu Devadhel\anaconda3\lib\site-packages\imblearn\utils\ validatior
=0.85 as keyword args. From version 0.9 passing these as positional arguments wil
warnings.warn(

9. Fit Samples -
® Fit samples to X_New_Under and Y_New_Under,and then check value count of target feature.

In [4@]: X New Under,¥Y New Under=obj.fit resample(X,Y)

In [41]: Y New Under.value counts()

Out[41]: @ 284315
i 5 213236
Name: Class, dtype: int64

Synthetic Minority Oversampling Technique (SMOTE)-

1.Problem Statement-

® While dealing with classification problems the percentage of classes in the total samples an
important role however there are scenarios you may be dealing with imbalance dataset i.e
Presence of minority class of in dataset.

2.Challenges Related to imbalanced Datasets-

® Biased Predication.

® Misleading Accuracy.
3.Some Examples-

® Credit cards frauds.

® Manufacturing Defects.

® Rare diseases diagnosis.

® Natural Disaters.

® Enrolment to premier institutes.
4.Solution-

® For dealing with imbalanced dataset on of the way is to resample the dataset by either
decreasing the the majority class or increasing minority class observations.
® SMOTE-It creates new synthetic observations or new datapoints .some are the steps below,

1)Identify the feature vector and its nearest neighbour.

2)Take the difference between two.

3)Multiply the difference with random number by in between 0 and 1.

4)Identify the new point on the line segment by adding random number to feature vector.
5)Repeat the process for identified feature vector.

Missing Values-

® |f there are missing values in the Dataset before doing any statistical analysis, we need to

handle those missing values.

® There are mainly three types of missing values.
1. MCAR(Missing completely at random): These values do not depend on any other features.
2. MAR(Missing at random): These values may be dependent on some other features.

3. MNAR(Missing not at random): These missing values have some reason for why they are

missing.

® Find Actions to be taken according Null values percentage in Feature-

1. 80to 90 % Null values- Drop feature.

2. 40to 60 % Null values- Build internal machine learning model for fill null values.
3. 20to 40 % Null values- Use Statistical techniques- Mean,Median and Mode.

1.Read data -

In [43]: df=pd.read_csv( Bengaluru House Data.csv')

It is not hard code rule,just good practice for data analysis.
In [44]: df

out[44]:
area_type availability location size society total_sqgft bath balcony price
0 Super built-up Area 19-Dec Electronic City Phase II 2BHK Coomee 1056 20 10 3907
1 Plot Area Ready To Move Chikka Tirupathi 4 Bedroom Theanmp 2600 50 30 120.00
2 Built-up Area Ready To Move Uttarahalli 3 BHK NaN 1440 20 30 6200
3 Super built-up Area Ready To Move Lingadheeranahalli 3BHK  Solewre 1521 30 10 9500
4 Super built-up Area Ready To Move Kothanur 2 BHK NaN 1200 20 10 51.00

2.Check shape of DataFrame-

In [46]: df.shape

out[46]: (13320, 9)

3.Check Null values of each feature-

In [47]: df.isnull().sum()

Out[47]: area type 5]
availability 5]
location 1
size 16
society 5502
total sqft 4]
bath 73
balcony 689
price 4]

dtype: inte4
® We can observed that there is 5 feature in dataset which contains null values,and we have to fill
or drop null values features according to scenario.

4.Drop society feature -
® We are dropping society feature from Dataframe.

Tn [48]: df_drop_society=df.drop( society’,axis=1)

In [49]: df _drop_society

Out[49]:
area_type availability location size total_sqft bath balcony price
0 Super built-up Area 19-Dec Electronic City Phase ll 2 BHK 1056 20 1.0 39.07
1 Plot Area Ready To Move Chikka Tirupathi 4 Bedroom 2600 50 3.0 120.00
2 Built-up Area Ready To Mave Uttarahalli 3 BHK 1440 20 30 6200

3 Super built-up Area Ready To Mave Lingadheeranahalli 3 BHK 1521 30 1.0 9500
5.Fill null values for feature-
® We are dropped society from dataframe, but still we have 4 features which have null values.
® Now we have to fill user defined/customized/mean values in balcony feature.

In [52]: df _drop_society['balcony'].unique()

Out[52]: array([ 1., 3., nan, 2., ©.])
In [57]: df drop society['balcony']=df drop society[ 'balcony'].fillna(df drop society['balcony'].mean())

In [58]: df _drop_society

Out[58]:
area_type availability location size total_sqft bath balcony price
0 Super buili-up Area 19-Dec Electronic City Phase Il 2 BHK 1056 2.0 1.000000 3907
1 PlotArea Ready To Move Chikka Tirupathi 4 Bedroom 2600 5.0 3.000000 120.00
2 Built-up Area Ready To Mave Uttarahalli 3BHK 1440 2.0 3.000000 6200
3 Super built-up Area Ready To Mave Lingadheeranahalli 3BHK 1521 3.0 1.000000 9500

® Now we have 3 feature which contains null values.

In [59]: df drop society.isnull().sum()

Out[59]: area type 2)
availability 4]
location 1
size 16
total sqft 4]
bath 73
balcony 5]
price 4]

6.Check feature have proper formatting-
® Check data types of all features,

In [63]: df drop _society.dtypes

wt[63]: | area type object
availability object
location object
size object
total sqft object
bath float64
balcony floated
price floated

® We can observed that total_sqft should be have int data type but due to improper data entry it
have object(string) data type.

® so we have to change data type of total_sqft, use linear regression for finding mean of range of
sqft feet.

In [64]: df _drop_society[ total sqft'].unique()
Out[B4]: array(['1056",

'2600', '1440°, ..., "1133 - 1384",

dtype=object)

In [82]: import re
def test(x):

obj=re.findall(r'\d+',x)

if len(obj)>1:

return (int(obj[@])+int(obj[1]))/2

return obj[8]

TE

'4689'1,

In [83]: df drop society['total sqft new']=df drop society[ total sqft'].apply(test)

In [84]: df _drop_society

Out[84]:

area_type availability location size total_sqft bath balcony price total_sqft new
0 Super built-up Area 19-Dec Electronic City Phase ll 2 BHK 1056 2.0 1.000000 39.07 1056
1 Plot Area Ready To Move Chikka Tirupathi 4 Bedroom 2600 50 3.000000 120.00 2600
2 Built-up Area Ready To Move Uttarahalli 3 BHK 1440 2.0 3.000000 62.00 1440
3 Super built-up Area Ready To Move Lingadheeranahalli 3BHK 1521 3.0 1.000000 95.00 1521
4 Super built-up Area Ready To Move Kothanur 2BHK 1200 2.0 1.000000 51.00 1200

® We have cleaned data from feature total_sqft and make new column as total_sqft_new.

® Now we can change data type of total_sqft_new as int,

In [89]: df_drop_society.dtypes

Out[89]: area type
availability
location
size
total sgft
bath
balcony
price
total sqft _new
dtype: object

object
object
object
object
object
floated
floate4d
floated
object

In [94]: df_drop_society[' total sqft new']=df_drop society['total sqft new'].astype('int')

In [95]: df_drop_society.dtypes

Out[95]: area_type
availability
location
size
total sqft
bath
balcony
price
total sqft new

object
object
object
object
object
floated
floated
floate4d
int32

Handling categorical Values-

® As computer has its own language, machine learning algorithms work on numerical data.

Whenever we have categorical data we have to Encode this data for better understanding to
algorithm and achieve optimum accuracy from model.

® So have below Encode techniques to handle categorical data and make it useful for the
machine learning algorithm to get insightful information.
1.0ne -Hot Encoding-

® One-Hot Encoding is a very handy and popular technique for treating categorical features. This

is based on creating additional features by its unique values.

® |n Python it can be implemented as:

In [31]: df
Out[31]:
Loan_ID Gender Married Dependents Education Self Employed Applicantincome Coapplicantincome LoanAmount Loan_Amount Term Credit _Hist
0 LP001032 Male No 0 Graduate No 4950 0.0 125 360
1 LP001824 Male Yes 1 Graduate No 2882 1843.0 123 480
2 LP002928 Male Yes 0 Graduate No 3000 3416.0 56 180
3 LPOO1814 Male Yes 2 Graduate No 9703 0.0 112 360
4 LP002244 Male Yes 0 Graduate No 2333 2417.0 136 360

® Use get dummies class from pandas -

[n [44]: col=[col for col in df if df[col].dtypes=='object']

In [45]: pd.get dummies(df[col])

Jut[45]:
Jependents_3+ Education_Graduate Frucation Ho Self_Employed_No Self_Employed_Yes Property _Area_Rural Property _Area_Semiurkan Property_Area_Urban
0 1 0 1 0 0 0 i
0 1 4] 1 0 0 1 [o]
0 1 0 1 0 0 1 0
0 1 0 1 0 0 0 1
® in above example we have encode all features which have object data type.

® Assumptions:

1. There are finite set of features.

2. Where no ordinal relationship exists between the categories of variable.
Advantages:

1) Easy to use.

2) Creates no bias as assumption of any ordering between the categories.
Disadvantages:

1) Can result in an increase in number of features resulting in performance issues.

2}It Increases Feature space.

2.0rdinal Number Encoding-

® This is easiest way and used in most of the data where there is natural relation between the
categories of ordinal values.

In [ J]: from sklearn.preprocessing import LabelEncoder
In [ ]: obj=LabelEncoder()
‘n [60]: df['Property Area Ordinal']=obj.fit_transform(df[ Property Area'])

‘n [63]: df['Property Area Ordinal'].value counts()

wt[63]: 1 146
2 125
n [64]:

ut[64]:

0 113
Name: Property Area Ordinal, dtype: int64

df[ "Property Area'].value counts()

Semiurban 146
Urban 125
Rural 113
Name: Property Area, dtype: int64

® We can observed that there are 3 categories in 'Property_Area';so 0,1,2 label allocated as per

alphabetical order of categories.
* Assumptions:
1. The integer values are having natural ordered relationship between each other. Like
Current>Ex>Never.
® Advantages:
1) Easy to use
2) Easily reversible.
3) Doesn't increase feature space.
® Disadvantages:
1) May result in unexpected results if the ordering of number is not related in any order.

3.Count Or Frequency Encoding-

® In

this type of encoding the count of existence of each category in the variable is determined.

Each category is then replaced by the frequency of it.

In [69]:

In [78]:

Ln [791]:

In [81]:

Out[81]:

In [82]:

Out[82]:

Property Area Count=df[ Property Area'].value counts().to_dict()

df[ ‘Property Area Count']=df['Property Area'].map(Property Area Count)

df.set_index('Property Area Count',inplace=True)

df['Property Area'].value counts()

Semiurban 146
Urban 125
Rural 113
Name: Property Area, dtype: inté64

df

Loan_ID Gender Married Dependents Education Self Employed Applicantincome C
Property_Area_Count

125 [P001032 Male No 0 0 No 4950
146 LP001824 Male Yes 1 0 No 2882
146 LP002928 Male Yes 0 0 No 3000
125 LP0O01814 Male Yes 2 0 No a703

® Assumptions:

There is no category of variable having similar frequency.
® Advantages
1. Easy to use.

DM Canih: vaiavaikla
Cadsily reversivie.
Doesn't increase feature space.
Disadvantages

He WwW Bb

Will not be able to handle if frequencies are same for two or more categories.

Outlier Removing Methods-

® An outlier is an observation that lies abnormally far away from other values in a dataset.
Outliers can be problematic because they can affect the results of an analysis.

A) How to Identify Outliers From data set-

® Before you can remove outliers, you must first decide on what you consider to be an outlier.
There are two common ways to do so:

1) Use the interquartile Range-

® The interquartile range (IQR) is the difference between the 75th percentile (Q3) and the 25th
percentile (Q1) in a dataset. It measures the spread of the middle 50% of values.

® You could define an observation to be an outlier if it is 1.5 times the interquartile range greater
than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1).

® Formula-

Outliers = Observations > Q3 + 1.5*IQR or Observations < Q1 - 1.5*IQR

2) Use z-scores.

® A z-score tells you how many standard deviations a given value is from the mean. We use the
following formula to calculate a z-score:
z={X-p)/o
where:
Xis a single raw data value
is the population mean
o is the population standard deviation
® You could define an observation to be an outlier if it has a z-score less than -3 or greater than 3.
Outliers = Observations with z-scores > 3 or <-3

B) How to Remove Outliers form Dataframe-

® Once you decide on what you consider to be an outlier, you can then identify and remove them
from a dataset.
l.Interquartile Range Method-
® (Create DataFrame-

In [2]: |#Create Dataframe with three columns A,B,C

In [3]: np.random.seed(10)
df = pd.DataFrame(np.random.randint(®, 10, size=(100, 3)), columns=['A', 'B', 'C'])

In [4]: df.head(10)

Out[4]:
AB C

0 9 4 0
1 1 9 0
2 1

8 9

® Find Q1,Q2 and Interquartile range for each column-

In [5]: from scipy.stats import stats
In [6]: Ql=df.quantile(q=8.25)
In [7]: Q3=df.quantile(q=8.75)

In [8]: Qrtl Range=df.apply(stats.iqr) #Inter Quartile Range

# 1st Quadrant

# 3rd Quadrant

In [9]: Clean Data=df[~((df<(Q1-1.5*Qrtl Range)) | (df>(Q3+1.5*Qrtl Range))).any(axis=1)]

In [11]: Clean Data

Out[11]:

® | ike this we can find outliers and remove from data set.

B
4
9

=]

0
0

0

2.Z- Score Method-

® (Create DataFrame-

In [2]: #Create Dataframe with three columns A,B,C

In [3]: np.random.seed(10}
df = pd.DataFrame(np.random.randint(@, 10, size=(108, 3)), columns=['A’

In [4]: df.head(16)

Out[4]:
A

0 9
1 1
2 1

B
4
9
8

® find absolute value of z-score and keep rows in dataframe with all z-scores less than absolute

value of 3

In [138]: #find absolute value of z-score for each observation
z = np.abs(stats.zscore(df))

In [131]: #only keep rows in dataframe with all z-scores less than absolute value of 3
data_clean

df[(z<3).all({axis=1)]

In [132]: #find how many rows are left in the dataframe

data_clean.shape

Out[132]: (lee, 3)

C) When To Remove Outliers-
® |f one or more outliers are present in your data, you should first make sure that they’re not a
result of data entry error. Sometimes an individual simply enters the wrong data value when
recording data.

® |[fthe outlier turns out to be a result of a data entry error, you may decide to assign a new value
to it such as Mean or Median of the dataset.

¢ |fthe value is a true outlier, you may choose to remove it if it will have a significant impact on
your overall analysis. Just make sure to mention in your final report or analysis that you
removed an outlier.

Machine Learning Algorithms-

® Machine Learning algorithms are the programs that can learn the hidden patterns from the
data, predict the output, and improve the performance from experiences on their own.

® Different algorithms can be used in machine learning for different tasks, such as simple linear
regression that can be used for prediction problems like stock market prediction, and the
KNN algorithm can be used for classification problems.

1.Linear Regression-

® |inear regression is a statistical model used to predict the relationship between Independent
and dependent variables.

® Linear regression makes predictions for continuous/real or numeric variables such as sales,
salary, age, product price, etc.

® Linear regression algorithm shows a linear relationship between a dependent (y) and one or
more independent (X) variables, hence called as linear regression. Since linear regression shows
the linear relationship, which means it finds how the value of the dependent variable is
changing according to the value of the independent variable.

YA

QL

yr. ®

E.

S

re

c

V

©

o

a Line of

© regression
independent Variables X

® Regression-Regression analysis is form of predictive modelling techniques which investigates
the relationship between a dependent and independent variable.

Types of Linear Regression
® Linear regression can be further divided into two types of the algorithm:
1.Simple Linear Regression:

® [fa single independent variable is used to predict the value of a numerical dependent variable,

then such a Linear Regression algorithm is called Simple Linear Regression.
2.Multiple Linear regression:

® |f more than one independent variable is used to predict the value of a numerical dependent
variable, then such a Linear Regression algorithm is called Multiple Linear Regression.

Basic Terminology in Linear Regression-
1.Linear Regression Line

® A linear line showing the relationship between the dependent and independent variables is

called a regression line. A regression line can show two types of relationship:
A)Positive Linear Relationship:

® [f the dependent variable increases on the Y-axis and independent variable increases on X-axis,
then such a relationship is termed as a Positive linear relationship.

¥
A

+ve line of regression

» X
The line equation will be: Y= ag+aix

B)Negative Linear Relationship:

® |fthe dependent variable decreases on the Y-axis and independent variable increases on the X-

axis, then such a relationship is called a negative linear relationship.
di

-ve line of regression

PX
The line of equation will be: Y= -agt+aiX

2.Feature spaces-

® js a collection of Independent and Dependent feature.
3.ML Models-

® Models are nothing but lines,surfaces,hyper surfaces in your feature space.
4.Line equation -
® Y=mX+c it tells you how X and Y are related to each other. m is slope, is Dependent feature, X is
independent variable.When you give data to Model it will find m and C for you and collectively
called as coefficients of models.

5. Collinearity-

® In linear regression model assumes that each dimensions are independent to other but in but in
reality they influence each other that problem is called Collinearity.

® Before building Model we have to check relation between each independent feature to the
target feature.Those features are strongly related to target feature only these features we have
to consider for analysis.

6.Coefficient of Co-relation-

® To measure strength of relation between independent variable and target variable for that we
use matric called Coefficient of Co-relation.

7.Covariance-

® Covariance is a measure of the relationship between two random variables. Covariance
between Independent and Target feature should be strong but Covariance between
independent variables should be zero.

® We tested R values for every independent feature with the Target variable and we select only
those whose R values either close to - 1 or +1. and those dimensions which have close value to 0
are useless dimensions for analysis in Linear Models.

8.How actually Algorithms Woks..??

® Algorithms start with some random m and c, and it will evaluate different possible lines before
its find best fit line.

9.What is Best Fit Line..?

® Amongst evaluated different possible lines the line goes through maximum data points and
minimizes distance between other points and line called as Best Fit Line.

® When working with linear regression, our main goal is to find the best fit line that means the
error between predicted values and actual values should be minimized. The best fit line will
have the least error.

® The different values for weights or the coefficient of lines (m) gives a different line of regression,
so we need to calculate the best values for m to find the best fit line, so to calculate this we use
cost function.

10.Error-

® distance between other points and line called as Errors.

0

200
Y
150 rn.
“ Emor = (T — (mx + C)
100 Sum of all errors can cancel
out and give 0
2

We square all the errors and
sum it up. That line which
0 © 100 rE 0 ET) PEG 15 RnR Sun ok squared

11.Mean Squared Errors-
® For Linear Regression, we use the Mean Squared Error (MSE) cost function, which is the
average of squared error occurred between the predicted values and actual values.
12, Residuals-
® The distance between the actual value and predicted values is called residual. If the observed
points are far from the regression line, then the residual will be high, and so cost function will
high. If the scatter points are close to the regression line, then the residual will be small and
hence the cost function.
12.0bjective Of Model-
® The algorithm will find out Best Fit Line from the infinite number of possibilities which have
minimum Sum Of Squared Error with the help of process called as Gradient Descent.
13.Gradient Descent-
® Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.
® Aregression model uses gradient descent to update the coefficients of the line by reducing the
cost function.
® [tis done by a random selection of values of coefficient and then iteratively update the values
to reach the minimum cost function.
® Internally makes use of partial derivatives ,partial derivative is nothing but dy/dx.For small
changing in m whether it is positive or direction from random m i.e dx and with respective m
changes what is behaviour of error i.e dy calculated.
® We are just doing with m, because we use mathematical term Partial derivative.we can also
calculate error w.r.t C. It finds out to the next line to the next line until it reaches global
minima.and this point it will get best fit line.

ik
Jw) — / Gradient
Global cost minimum
— J n(W)
»
w
14.Learning Step-

® |n process of jumping one to another for finding best fit line there is concept Learning Step. As
jump towards your goal the steps become smaller and smaller this algorithm is called as Bold
Driver algorithm and is part of Gradient Descent.

15.Coefficient Of Determination/R squared-

® Once we build a model we have to see how good the model is or how reliable the model is for
that we use another matric called as Coefficient of Determination.and this is represented as R
squared. R squared ranges between 0 and 1.

® The Goodness of fit determines how the line of regression fits the set of observations. The
process of finding the best model out of various models is called optimization. It can be
achieved by R Squared Method.

® |t measures the strength of the relationship between the dependent and independent variables
on a scale of 0-100%.

® R-Squared is a ratio between variance explained by your model to the total variance in the
datapoint

® R-Squared value is a statistical measure of how close the are to the fitted regression line.

® High R squared value is considered as good linear model.

17.Assumptions-

1.Assumption Of Linearity-

® Assumes a linear relation between the dependent/target variable and the independent
/predictor variables.

2.Assumption of normality of the error distribution-

® The errors should be normally distributed across the model.

3.Small or no Collinearity between the features-

® The model assumes either little or no collinearity between the features or independent
variables.

18.Advantages and Disadvantages of Linear Regression-

Linear Regression Model -

Advantages —
1. Simple to implement and easier to interpret the outputs coefficients

Disadvantages -
1. Assumes a linear relationships between dependent and independent variables. That
is, it assumes there is a straight-line relationship between them

2. Outliers can have huge effects on the regression
3. Linear regression assume independence between attributes

4. Linear regression looks at a relationship between the mean of the dependent variable
and the independent variables.

5. Just as the mean is not a complete description of a single variable, linear regression
is not a complete description of relationships among variables

6. Boundaries are linear

19.Application Of Linear regression in Business-

® Evaluating Trends and Sales Estimates.

® Analyzing the impact of price changes.

® Assessment of risk in financial services and insurance domain.

What is Ensemble Method..??
® An ensemble method is a technique which uses multiple independent similar or different
models/weak learners to derive an output or make some predictions. For e.g. A random forest is
an ensemble of multiple decision trees. An ensemble can also be built with a combination of
different models like random forest, SVM, Logistic regression etc.

Implementation Of Linear Regression-
1.Reading Data-

In [1]: import pandas as pd

In [2]: dmport seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rcParams['figure.figsize']=(20.0,10.0)

In [21]: df=pd.read csv('C:\\Users\Abhimanyu Devadhe\Desktop\\iris.csv')

In [23]: df.head()

a Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species
0 1 51 35 1.4 0.2 Ins-setosa
1 2 49 3.0 1.4 0.2 Ins-setosa
2 3 4.7 32 1.3 0.2 Ins-setosa
3 4 46 1 je 02 Iris-setosa

2.Separating Data in Dependent and Independent Variable-
® Considering Sepal length as Dependant variable and Petal Length as In dependant variable.

In [29]: y=df[["SepallengthCm"]]
In [32]: X=df[["PetallengthCm"]]

In [42]: X.head(2)

out[42]:
PetalLengthCm
0 14
1 14

In [43]: y.head(2)

Out[43]:
SepalLengthCm
0 51
1 49

3.Divide dataset X and y for Training set and Testing Set-

In [44]: from sklearn.model selection import train test split
In [45]: X train,X test,y train,y test=train test split(X,y,test size=0.3)

® We have to import package train_test_split from library sklearn.model_selection.
® we have to store result in four variables as X_train,X_test,y_train and y_test.

4, Build a Linear regression Model-

In [48]: from sklearn.linear_model import LinearRegression
In [49]: LR _Model=lLinearRegression()
In [58]: LR Model.fit(X train,y train)
Out[50]: LinearRegression()
® Import package LinearRegression from sklearn.linear_model.
5.Check Coefficient of Intercept(c) and Coefficient of slope(m)-
In [6@]: LR Model.intercept

Out[6@]: array([4.25898748])

In [61]: LR Model.coef

Out[61]: array([[@.41615084]])

6.Predict Output-
In [51]: y_Pred=LR_Model.predict(X_test)

In [57]: y test.head(5),y Pred[@:5]

out[57]: ¢( SepallengthCm
81 5.5
Q 5.1
64 5.6
63 6.1
148 6.2,

array ([[5.79874561],
[4.84159866],
[5.75713052],
[6.21489645],
[6.5062020411))

® Predicty_Pred top on X_test.

7.Find error in prediction-

In [58]: from sklearn.metrics import mean squared error

In [59]: mean squared error(y_test,y Pred)

Outl5971: ©.1525140754R635873
———p ==" PE PE TE BR PP

We have to find mean_squared_error for checking accuracy of model,so in that case we
observe mean_squared_error is less about 0.152.

2. Logistic Regression-

Logistic Regression is one of the most popular machine learning algorithms used for binary
classification.

Logistic regression is one of the most popular Machine Learning algorithms, which comes under
the Supervised Learning technique. It is used for predicting the categorical dependent variable
using a given set of independent variables.

Logistic regression predicts the output of a categorical dependent variable. Therefore the
outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False,
etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie
between 0 and 1.

Logistic Regression is much similar to the Linear Regression except that how they are used.
Linear Regression is used for solving Regression problems, whereas Logistic regression is used
for solving the classification problems.

In Logistic regression, instead of fitting a regression line, we fit an "S" shaped logistic function,
which predicts two maximum values (0 or 1).

The curve from the logistic function indicates the likelihood of something such as whether the
cells are cancerous or not, a mouse is obese or not based on its weight, etc.

Logistic Regression is a significant machine learning algorithm because it has the ability to
provide probabilities and classify new data using continuous and discrete datasets.

Logistic Regression can be used to classify the observations using different types of data and
can easily determine the most effective variables used for the classification. The below image is
showing the logistic function:

A

0.5

Threshold Value

y=0.3

Px

Note: Logistic regression uses the concept of predictive modeling as regression; therefore, it is
called logistic regression, but is used to classify samples; Therefore, it falls under the
classification algorithm.
Logistic Function (Sigmoid Function):

© The sigmoid function is a mathematical function used to map the predicted values to

probabilities.
© It maps any real value into another value within a range of 0 and 1.
© The value of the logistic regression must be between 0 and 1, which cannot go beyond this
limit, so it forms a curve like the "S" form. The S-form curve is called the Sigmoid function
or the logistic function.
© In logistic regression, we use the concept of the threshold value, which defines the
probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value
below the threshold values tends to 0.
Assumptions for Logistic Regression:
© The dependent variable must be categorical in nature.
© The independent variable should not have multi-collinearity.
Logistic Regression Equation:
® The Logistic regression equation can be obtained from the Linear Regression equation. The
mathematical steps to get Logistic Regression equations are given below:
® We know the equation of the straight line can be written as:

y=bg + Dix; + boxy + baxg +--+ bpyxy

® |n Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation
by (1-y):
= : 0 for y= 0, and infinity for y=1
ER
® But we need range between -[infinity] to +[infinity], then take logarithm of the equation it will
become:

log i =bg + byxy + boxy + byxs + + b,x,

® The above equation is the final equation for Logistic Regression.
Type of Logistic Regression:
® On the basis of the categories, Logistic Regression can be classified into three types:
o Binomial: In binomial Logistic regression, there can be only two possible types of the
dependent variables, such as 0 or 1, Pass or Fail, etc.
© Multinomial: In multinomial Logistic regression, there can be 3 or more possible
unordered types of the dependent variable, such as "cat", "dogs", or "sheep"
© Ordinal: In ordinal Logistic regression, there can be 3 or more possible ordered types of
dependent variables, such as "low", "Medium", or "High",

Evaluation Of Classification Model-

® Logistic Regression employs all different sets of metrics. Here, we deal with probabilities and
categorical values.

® Confusion matrix is the most crucial metric commonly used to evaluate classification models.
It's quite confusing but make sure you understand it by heart. The skeleton of a confusion
matrix looks like this:

1 0
(Predicted) (Predicted)

1
True Positive | False Negative
(Actual) (TP) (FN)
0 False Positive | True Negative
(Actual) (FP) (TN)

As you can see, the confusion matrix avoids "confusion" by measuring the actual and predicted
values in a tabular format. In table above, Positive class = 1 and Negative class =0.

Following are the metrics we can derive from a confusion matrix:

Accuracy - It determines the overall predicted accuracy of the model. It is calculated

as Accuracy = (True Positives + True Negatives)/(True Positives + True Negatives + False
Positives + False Negatives)

True Positive Rate (TPR) - It indicates how many positive values, out of all the positive values,
have been correctly predicted. The formula to calculate the true positive rate is (TP/TP + FN).
Also, TPR= 1 - False Negative Rate. It is also known as Sensitivity or Recall.

False Positive Rate (FPR) - It indicates how many negative values, out of all the negative
values, have been incorrectly predicted. The formula to calculate the false positive rate is (FP/FP
+ TN). Also, FPR = 1 - True Negative Rate.

True Negative Rate (TNR) - It indicates how many negative values, out of all the negative
values, have been correctly predicted. The formula to calculate the true negative rate is (TN/TN
+ FP). It is also known as Specificity,

False Negative Rate (FNR) - It indicates how many positive values, out of all the positive
values, have been incorrectly predicted. The formula to calculate false negative rate is (FN/FN +
TP).

Precision: It indicates how many values, out of all the predicted positive values, are actually
positive. It is formulated as:(TP / TP + FP).

F1 Score: F score is the harmonic mean of precision and recall. It lies between 0 and 1. Higher
the value, better the model. It is formulated as 2((precision*recall) / (precision+recall)).

ROC curve-ROC and AUC curve basically used for to fix threshold value and Plot in between
True Positive Rate(Y axis) and False Positive Rate(X axis).With tunning threshold values we
obtain datapoints and combinedly plotted on graph.Threshold values are fix w.r.t to domain
and required severity of TPR and FPR only.

AUC -Area Under curve,Area under ROC curve called as AUC And the more the area under the
curve the better the model is yours.

Why Is Logistic Regression Called "Logistic Regression" and Not "Logistic Classification"?
Logistic regression uses the concept of predictive modeling as regression; therefore, it is called
logistic regression, but is used to classify samples; Therefore, it falls under the classification
algorithm.

Logistic regression borrows its name from the logistic function and linear regression algorithm.
Linear regression does not work well with classification problems.
Logistic regression uses the logistic function which squashes the output range between 0 and

1 and makes 11se of hvnathecis fiinction of the linear resrescinn aloorithm
EE ded BLED 2 Ea Ee EE RE ER oR ER TY SC RAS RT

Implementation Of Logistic Regression-
1.Import Data-

In [1]: import pandas as pd
import numpy as np

In [3]: df= pd.read csv("C:\\Users\Abhimanyu Devadhe\Desktop\\HR comma sep.csv")

In [4]: df
out[4]:
satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left p
0 0.38 0.53 2 157 3 0 1
1 0.80 0.86 5 262 6 0 1
2 01 0388 Id 272 4 0 1

2. Convert Categorical feature into Numerical feature-
® with the help of get_dummies function we can did 'one hot encoding’ to convert categorical
features in numerical feature.

In [17]: dfl=pd.get dummies(df.salary)

In [19]: df2=pd.get dummies(df.sales)

In [22]: df3=pd.concat([df,df1,df2],axis=1)

In [28]: | df Final=df3.drop(["sales”,"salary™],axis=1)

In [29]: df_Final

Qut[29]:
satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left
0 0.38 0.53 2 157 3 0 i
4 nan nos = MRD " n 4

® There is salary and sales are categorical features in our dataset we are converted into numerical
feature.

3.Split dataset in X and y as Independent and Dependent features-
In [31]: X=df Final.drop(["left"],axis=1)
In [33]: Y=df Final[["left"]]
® |eft column is considered as dependent feature and others are independent feature.
4.Split X and Y for Training and Testing-

In [41]: from sklearn.model selection import train_test_ split

In [42]: X_train,X test,y train,y test=train_test split(X,Y,test size=0.2)

5. Build Model-
In [44]: from sklearn.linear_model import LogisticRegression
In [46]: LR Model=lLogisticRegression()

In [47]: LR Model.fit(X train,y train)

C:\Users\Abhimanyu Devadhe\anaconda3\lib\site-packages\sklearn\utils\validation.p
v was passed when a 1d arrav was expected. Please change the shape of v to (n sam

® from sklearn.linear_model import LogisticRegression.

6.Predict y_test w.r.t X_test-
In [49]: vy _pred=Model.predict(X test)

In [56]: len(y pred)

Out[56]: 3060

In [55]: len(y_ test)

Out[55]: 3000

7.Evaluation of Model-
i)Confusion Matrix-

In [58]: from sklearn.metrics import confusion_matrix,precision_score,recall_score,fl_score,accuracy_score
In [59]: em=confusion_matrix(y_test,y pred)

In [60]: cm

Out[6@]: array([[2099, 169],
[ 465, 267]], dtype=int64)

ii)accuracy_score-

In [61]: accuracy _score(y_ test,y pred)

Out[61]: ©.7886666666666666
iii)precision_score-

In [62]: precision _score(y_test,y pred)

Out[62]: ©.6123853211009175
iv)recall_score-

In [63]: recall score(y_test,y pred)

Out[63]: ©.36475409836065575
v)fl_score-

In [64]: f1 _score(y_test,y pred)

Out[64]: ©.45719178082191786

3.Desicion Tree-

f
|
|
|
|
|
|
|

1

Decision Tree is a Supervised learning technique that can be used for both classification and
Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-
structured classifier, where internal nodes represent the features of a dataset, branches
represent the decision rules and each leaf node represents the outcome.

In a Decision tree, there are two nodes, which are the Decision Node and Leaf Node. Decision
nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the
output of those decisions and do not contain any further branches.

The decisions or the test are performed on the basis of features of the given dataset.

It is a graphical representation for getting all the possible solutions to a problem/decision
based on given conditions.

It is called a decision tree because, similar to a tree, it starts with the root node, which expands
on further branches and constructs a tree-like structure.

In order to build a tree, we use the CART algorithm, which stands for Classification and
Regression Tree algorithm.

A decision tree simply asks a question, and based on the answer (Yes/No), it further split the
tree into subtrees.

Below diagram explains the general structure of a decision tree:

Note: A decision tree can contain categorical data (YES/NO)_as well as numeric data.

Ee SEE

Sub-Tree

Decision Node

Decision Node

v v

Leaf Node Leaf Node

v v

Leaf Node Decision Node

EEE |
\/ v

Leaf Node Leaf Node

a mm mm m= — — ——

Why use Decision Trees?

There are various algorithms in Machine learning, so choosing the best algorithm for the given
dataset and problem is the main point to remember while creating a machine learning model.
Below are the two reasons for using the Decision tree:

Naricinn Troaac nenallv mimic hitman thinkino ahilitv while malkine a dacicinn cn it ic aacu tn
¢ LLIDIVIL TTLL0 MQUUIILY THING HIUATTIUET WHITING GILILY UWI HUNTS © ULLIJIVIT OV TL 19 LUIy ww

understand.
. The logic behind the decision tree can be easily understood because it shows a tree-like
structure.

Decision Tree Terminologies

. Root Node: Root node is from where the decision tree starts. It represents the entire dataset,
which further gets divided into two or more homogeneous sets.

. Leaf Node: Leaf nodes are the final output node, and the tree cannot be segregated further
after getting a leaf node.

. Splitting: Splitting is the process of dividing the decision node/root node into sub-nodes
according to the given conditions.

4. Branch/Sub Tree: A tree formed by splitting the tree.
5. Pruning: Pruning is the process of removing the unwanted branches from the tree.
6. Parent/Child node: The root node of the tree is called the parent node, and other nodes are

called the child nodes.

How does the Decision Tree algorithm Work?

In a decision tree, for predicting the class of the given dataset, the algorithm starts from the
root node of the tree. This algorithm compares the values of root attribute with the record (real
dataset) attribute and, based on the comparison, follows the branch and jumps to the next
node.

For the next node, the algorithm again compares the attribute value with the other sub-nodes
and move further. It continues the process until it reaches the leaf node of the tree. The
complete process can be better understood using the below algorithm:

Step-1: Begin the tree with the root node, says S, which contains the complete dataset.

Step-2: Find the best attribute in the dataset using Attribute Selection Measure (ASM).

Step-3: Divide the S into subsets that contains possible values for the best attributes.

Step-4: Generate the decision tree node, which contains the best attribute.

Step-5: Recursively make new decision trees using the subsets of the dataset created in step -3.
Continue this process until a stage is reached where you cannot further classify the nodes and
called the final node as a leaf node.

Example: Suppose there is a candidate who has a job offer and wants to decide whether he
should accept the offer or Not. So, to solve this problem, the decision tree starts with the root
node (Salary attribute by ASM). The root node splits further into the next decision node
(distance from the office) and one leaf node based on the corresponding labels. The next
decision node further gets split into one decision node (Cab facility) and one leaf node. Finally,
the decision node splits into two leaf nodes (Accepted offers and Declined offer). Consider the
below diagram:

Salary is between
$50000-580000

Office near to

Declined
home Ceo
Provides Cab Declined
facility offer

Accepted Declined
offer offer

Attribute Selection Measures
® While implementing a Decision tree, the main issue arises that how to select the best attribute
for the root node and for sub-nodes. So, to solve such problems there is a technique which is
called as Attribute selection measure or ASM. By this measurement, we can easily select the
best attribute for the nodes of the tree. There are two popular techniques for ASM, which are:
A) Information Gain
B) Gini Index
1. Information Gain:
® [Information gain is the measurement of changes in entropy after the segmentation of a dataset
based on an attribute.
® It calculates how much information a feature provides us about a class.
® According to the value of information gain, we split the node and build the decision tree.
® Adecision tree algorithm always tries to maximize the value of information gain, and a
node/attribute having the highest information gain is split first. It can be calculated using the
below formula:

Information Gain= Entropy(S)- [(Weighted Avg) *Entropy(each feature)

® Entropy: Entropy is a metric to measure the impurity in a given attribute. It specifies
randomness in data. Entropy can be calculated as:

Entropy(s)= -P(yes)log2 P(yes)- P(no) log2 P({no)

Where,
e S=Total number of samples
® P(yes)= probability of yes
© P(no)= probability of no

Analytical Solution for finding Entropy for Dataset and Individual attribute.

——
mE De Decision Tyree — ID2 (Infovmakon geun )
| Pay outlook] Temp [Humidity Wind [play (iol?

BL | Suny | bod | High [Weak | Vo |
Da | Sunny | Hed | bigh Sherng | No |
| al D2 |pvercast! Het | Lawak | Wrak | ie

04 | Rain | Md | Hish [weak | MD |
BE | Rein | (oo) | nema) [weak | YD |
i D¢ Rain too) | Nevmal Long | Np |
DF |ovexad | (oo) |Newmal | LY | ye |
DE, Sunny | Mild HigN | weak | ND |
Pg | Sunmy| Cool |npymed weak | |

“eo |
Dio | Rain Mild Noymal | Wak | ve)
PI | Sunny mild NOX) 5 Kory) Yeo |

Dla [overas mid | High | GHeeng | yen |

DIB | overeat) Hot | Noro | Leak | Yen |

Oly | Rain | mid High | Story] Noo |

- FissHy we need to ander stand polach ote hole

giving manimum in Permalion oot of Avedlahle atfvy mds,
a? “0 InHuis Cane te have Rouse attwhden 54 Cit
att bubs we ped by cof clade infovmation gary,

~ Twe tRibade having mamimum, Inox tedion qed
oe Cordidee on 97 pu} Node, ine] Fo roa ol?
wi &resd +1 budding dee. :

dis TT ae
|@D bhsibute — outlook CL ae
Voluen Couteok) = {unny, puetcant, fern ——
- 1% dou Waut +o ca) cwhede ThPoy dahon oo————
OE oHEIRIE HGR toe have Fo corciafe fon

| oF wre dodoset pn Eno Entyop
he 1 OX ; p 0

————

ah) - neste, P = posit , Ne neg okie
| > Entop of EH vt Lockoryed( Ly
Sqr), s(n) —= Ploy (el P-
I 2 Mos SF _ 5. og, ©
{jg bs :

. Tr. Suns y + D- SumnY) pm x

ENFoPY _ Sunny LP puedes)

novercont) [pen x Enhepy.

H

| {orn x Enrepy_ vaio

/

o Bare Find Pov tach ali buke an $ollaos~
1Y Gain (outlsek 60.7¢C
1y Goin (Temp) - 00299
hy Gam Cuma ty 8518
M Gon (Wind) = 0-049

© 0dook ad vibe hate pan moum InYDy woe Han
gainy , 50 (ve (onsidet outlook han xeok node

classmate PAGE

2, Gini Index:
® Gini index is a measure of impurity or purity used while creating a decision tree in the
CART(Classification and Regression Tree) algorithm,
® An attribute with the low Gini index should be preferred as compared to the high Gini index.
® [tonly creates binary splits, and the CART algorithm uses the Gini index to create binary splits.

® Gini index can be calculated using the below formula:

Gini Index= 1- XjPj2

Pruning: Getting an Optimal Decision tree
® Pruning is a process of deleting the unnecessary nodes from a tree in order to get the optimal
decision tree.
® A too-large tree increases the risk of overfitting, and a small tree may not capture all the
important features of the dataset. Therefore, a technique that decreases the size of the learning
tree without reducing accuracy is known as Pruning. There are mainly two types of tree
pruning technology used:
1. Cost Complexity Pruning
2.Reduced Error Pruning.
® Advantages of Pruning a Decision Tree
1.Pruning reduces the complexity of the final tree and thereby reduces overfitting.

2.Explainability — Pruned trees are shorter, simpler, and easier to explain.

® Limitations of Pruning
1.Similar to Lasso regularization, there is no real disadvantage. However, pruning does come

with a high computational cost.
Advantages of the Decision Tree
® [tis simple to understand as it follows the same process which a human follow while making
any decision in real-life.
® [t can be very useful for solving decision-related problems.
® |t helps to think about all the possible outcomes for a problem.
® There is less requirement of data cleaning compared to other algorithms.
Disadvantages of the Decision Tree
® The decision tree contains lots of layers, which makes it complex.
® It may have an overfitting issue, which can be resolved using the Random Forest algorithm.
® For more class labels, the computational complexity of the decision tree may increase.

Implementation Of Decision Tree-
1.Load Dataset-

In [3]: import seaborn as sns

In [4]: df=pd.read_csv("golf-dataset.csv"

In [5]: |dFf
Out[5]:
Outlook Temp Humidity Windy Play Golf
0 Rainy Hot High False No
1 Rainy Hot High True No

® golf dataset is commonly used dataset for decision tree.

2.Check datatype of features-

In [7]: df.dtypes

Out[7]: Outlook object
Temp object
Humidity object
Windy bool

Play Golf object
dtype: object

In [24]: dfl=df["Windy"].astype("str")

In [28]: df2=df.drop(["Windy"],axis=1)

In [29]: df3=pd.cencat([dfl,df2],axis=1)

In [32]: df3.dtypes

Out[32]: Windy object
Outlook object
Temp object
Humidity object

Play Golf object
dtype: object

® datatype of should be string for encoding of features.

3.Split dataset into X and Y and Encode Features-

In [45]: X=df3.drop(["Play Golf"],axis=1) #Dependent feature is removed.
In [46]: X _Final=pd.get dummies(X) #0ne hot encoding for Independent Features.
In [48]: Y=df3[["Play Golf"]] #Considering Play Gold as Target Feature
In [55]: Y_Final=Y["Play Golf"].map({"Yes":1,"No":8}) #Encoding for Target feature

® play golf feature is our target feature and others are independent feature.

4.Split dataset for Training and Testing-

In [58]: X_train,X test,y Train,y test=train test split(X Final,Y Final, test size=0.15)

5. Build Model-
A) Build Model on Information Gain ASM-

In [59]: from sklearn.tree import DecisionTreeClassifier
In [69]: Entr_Model=DecisionTreeClassifier(criterion="entropy")
In [70]: DT _Model=Model.fit(X train,y Train)

® use criterion as entropy.
B) Build Model on Gini Index-

In [83]: Gini Model=DecisionTreeClassifier(criterion="gini")

In [84]: DT _Model2=Gini Model.fit(X train,y Train)

6.Predict Play golf w.r.t X_test for both model-

In [92]: y Predl=DT Modell.predict(X test)

In [93]: y_Pred2=DT_Model2.predict(X test)

7.Validation of Models-
A) For Information Gain Model-

In [1@@]: confusion matrix(y test,y Predl)

out[lee]: array([[1l, 1],
[@, 11], dtype=inte4)

In [101]: precision score(y test,y Predl)

Out[101]: ©.5

In [102]: accuracy score(y test,y Predl)
Out[102]:

In [163]:

Out[103]:

In [1604]:

Out[1e4]:

©.bb6666666b666666

recall score(y test,y Predl)

4.58

f1 score(y test,y Predl)

0.666666666b666666

B) For Gini Index Model-

In [185]:

Out[1@5]:

In [106]:

Out[1l@6]:

In [107]:

Out[107]:

In [188]:

Out[1@8]:

In [118]:

Out[11@]:

confusion _matrix(y test,y Pred2)

array([[1, 1],
[@, 1]], dtype=inte4)

precision score(y test,y Pred?)

0.5

accuracy score(y_test,y Pred2)

0.66666666606666606

recall score(y_test,y Pred2)

1.0

f1 score(y test,y Pred2)

0.66666666606666606

4. Random Forest-

® Random Forest is a popular machine learning algorithm that belongs to the supervised learning

technique. It can be used for both Classification and Regression problems in ML. It is based on
the concept of ensemble learning, which is a process of combining multiple classifiers to solve
a complex problem and to improve the performance of the model.

As the name suggests, "Random Forest is a classifier that contains a number of decision
trees on various subsets of the given dataset and takes the average to improve the
predictive accuracy of that dataset.” Instead of relying on one decision tree, the random
forest takes the prediction from each tree and based on the majority votes of predictions, and it
predicts the final output.

The greater number of trees in the forest leads to higher accuracy and prevents the
problem of overfitting.

Random Forest is basically based on ensemble technic called as bagging(Bootstrap
Aggregation).

Base Learner Prediction of Model
m no of rows

—> 1
Ne——
n no of colomns
i 1 DT Modell
I |) fr——
Dataset 1
Feature sample+Row Sample — > 1
— 1
Feature sample+Row Sample DEEEeED DT Model 2 Majaroty
m
D —
Feature sample+Row Sample TT —
Dataset 3 — > 0
Feature sample+Row Sample
SA
- DT Model 3
Dataset
D ——
Dataset 4
—> 1
DT Model 4

| | |
I Boostrap Zone ' Aggregation Zone |

® Observer above scenario,we have one dataset have n no of columns as m no of rows.

® And we have 4 no of models which are basically Decision tree.

® From dataset we will randomly pick some no of features and rows for each individual model
with the help of 'Row Sampling with Replacement’ method.

® With the help of provided dataset points model has trained and ready for prediction.

® Then we provide test data for testing of model and model predicts 0 and 1 w.r.t test data.
(assuming Binary classification).

® For final prediction we use Aggregation method,in which our final prediction will be majority
prediction of models.Majority is used for classification model.

® When we use RF for regression model use Mean or median at the place of majority.

® The below diagram explains the working of the Random Forest algorithm:

Training Training Training
Data Data Data

Voting
(averaging)

Assumptions for Random Forest
® Since the random forest combines multiple trees to predict the class of the dataset, it is
possible that some decision trees may predict the correct output, while others may not. But
together, all the trees predict the correct output. Therefore, below are two assumptions for a
better Random forest classifier:
© There should be some actual values in the feature variable of the dataset so that the
classifier can predict accurate results rather than a guessed result.
© The predictions from each tree must have very low correlations.
Why use Random Forest?
® Below are some points that explain why we should use the Random Forest algorithm:
o |t takes less training time as compared to other algorithms.
o |t predicts output with high accuracy, even for the large dataset it runs efficiently.
© [t can also maintain accuracy when a large proportion of data is missing.
How does Random Forest algorithm work?

® Random Forest works in two-phase first is to create the random forest by combining N decision
tree, and second is to make predictions for each tree created in the first phase.

® The Working process can be explained in the below steps and diagram:

Step-1: Select random K data points from the training set.
Step-2: Build the decision trees associated with the selected data points (Subsets).
Step-3: Choose the number N for decision trees that you want to build.
Step-4: Repeat Step 1 & 2.
Step-5: For new data points, find the predictions of each decision tree, and assign the new data
points to the category that wins the majority votes.

® The working of the algorithm can be better understood by the below example:

e Example: Suppose there is a dataset that contains multiple fruit images. So, this dataset is
given to the Random forest classifier. The dataset is divided into subsets and given to each
decision tree. During the training phase, each decision tree produces a prediction result, and
when a new data point occurs, then based on the majority of results, the Random Forest
classifier predicts the final decision. Consider the below image:

Instance

Nx Nu
NX eee

Tree-2 Tree-n
Ww &

Class-A Class-A Class-B

Applications of Random Forest

® There are mainly four sectors where Random forest mostly used:

1. Banking: Banking sector mostly uses this algorithm for the identification of loan risk.

2. Medicine: With the help of this algorithm, disease trends and risks of the disease can be
identified.

3. Land Use: We can identify the areas of similar land use by this algorithm.
4. Marketing: Marketing trends can be identified using this algorithm.

Advantages of Random Forest

® Random Forest is capable of performing both Classification and Regression tasks.
® [tis capable of handling large datasets with high dimensionality.
® [tenhances the accuracy of the model and prevents the overfitting issue.

Disadvantages of Random Forest

® Although random forest can be used for both classification and regression tasks, it is not more

suitable for Regression tasks.

@ Regrension ne (one + —
1]

Tr undefing J 1 IT ovekiing
Sah x XXX Panes
NC
x wm of AT pert
ks Polynomial 4 Poly romial = 0a A Plebromiat i

=
4
x]

Nr

——a

[CEE

© Dhgeswe abole diagrams [graphs

+ THs a polynomial ~Yeqvession S(enaviol

+ Observakbons F~om 4~aph

i> Medel is undes Pitting —> (ohatever oda +rained

by model oy is quite Wi'qh [ov had.

undexbitting is nothing bud accutacy Fes veining

do¥a ond 1esHRg ig low.

iy High bias € Pigh vauan ce

iS Aveiding Huis Hype of medel.

B mede) ig 0verBit — QverPrting noting Lud

actuzaty is Wigh als yauning dete hud loo

at tert dala,

iy Low bios § High vauance.

Wh Wousliy te avold “Had ype 6 > enY OV

@IGY — qveph
BS low biad $ low Laganie
ny Ty il gen egal zed medel , tually toe ate
erpecHng Huis type of model.
iS mode Shewld give dome accwtaty Pox H-2auning
dodood ow» We) an tesHng datoset,

Blown deinen an Gryovse ok the
reauning deka Set

¢ fox generalized ML mode . Model &hould he
low bran

® Votionte—> \ationte debined ap Exvove oF
Hud tent datgset (

» Fox genewr)i zed mu model, medel howd be
lol) vasiance.

« pecisiontate disguised wow oF tree jteelR
Completely to Lut depths, 188 takes ai) Fe attsea
ond then [Ru <heuke ¢plithing to IR tomplefc
Aepth. »

« In 07 auwacy al g taining datane 15 foo
Figh bul od sesbing 4 eM be 10w , so DT Madef

bav<  0Vee iting sine and baw Yolo bios and
[Man vasiance .

® Pardom Fores) 9

+ In Random Povest (pe bawve mu Hple pT ip
Parallel,

«In Rondon Povest | We age bing Multiple DT and
have loo hicw and igh vatiance each ,

+» Ih RY Lt hawt (nl Noot chvap Pggregakion
techmique and bawe mwHple oT in patalled , «

. Go Wigh valance Convetked inty 160 vadiance
e DC haw mn hion ahd ad vale ce |

== [5 bee 2 LAs Nee ey I ne —

®)

WS Ensembles — Entemmblen means Combining
Mw Hple ML pode Boy fipad predicHon.
'Thete ate ttos +echniquer in Entenbles

py Ragqging £ £7 BsoeHng |

« Alto Called ov boosttvap Aggregation.
e Tas teebnijque wed in Randsny, Foveot.

= Nhe 6% I Mi—— J
Feature, LIE A

bw]
ii? [ (=
OF; 2 . Nada ity |
a3 Ta [Le :
+ ]
< sk Datosed No Mr
D AN 0
Ni
- Rl
_— + - RI 3
| bh 0) RNaseseac kan
= Boors~ap =z Seed
ma 00 ap Lone Zone
| * We hove a datowd D and wits Hx help op

-—

Pew Sar ling toith Re pla ceme uf © we Bee PNpuide
_wandom daba” bs each meele], Cin
* Mode] oh) 4vain ahd give Predictor IS

ER aT

oe Tlun we Pind maioety Pom that and Hes
ginal pediceton <

Clee

Cross Validation in ML-

® |n machine learning, we couldn’t fit the model on the training data and can’t say that the model
will work accurately for the real data. For this, we must assure that our model got the correct
patterns from the data, and it is not getting up too much noise. For this purpose, we use the
cross-validation technique.

® Cross-validation is a technique in which we train our model using the subset of the data-set and
then evaluate using the complementary subset of the data-set.

® Methods of Cross Validation-

1.Validation

® In thie mathnd wa narfarm trainine nn tha RN0A nf tha ivan data-cat and ract BNA ic cad far
THAD IHR IW YOO OT IT LUIS, WIT LT JU JV UT LIL SIVA UULU JLL UHTW TLIL WV /V 19 UIs Tw

the testing purpose. The major drawback of this method is that we perform training on the 50%
of the dataset, it may possible that the remaining 50% of the data contains some important
information which we are leaving while training our model i.e higher bias.

2.LOOCV (Leave One Out Cross Validation)

® |n this method, we perform training on the whole data-set but leaves only one data-point of the
available data-set and then iterates for each data-point. It has some advantages as well as
disadvantages also.An advantage of using this method is that we make use of all data points
and hence it is low bias.The major drawback of this method is that it leads to higher variation in
the testing model as we are testing against one data point. If the data point is an outlier it can
lead to higher variation. Another drawback is it takes a lot of execution time as it iterates over
‘the number of data points’ times.

3.K-Fold Cross Validation-

® |n this method, we split the data-set into k number of subsets(known as folds) then we perform
training on the all the subsets but leave one(k-1) subset for the evaluation of the trained model.
In this method, we iterate k times with a different subset reserved for testing purpose each time.

® Note-It is always suggested that the value of k should be 10 as the lower value of k is takes
towards validation and higher value of k leads to LOOCV method.

® Example-The diagram below shows an example of the training subsets and evaluation subsets
generated in k-fold cross-validation. Here, we have total 25 instances. In first iteration we use
the first 20 percent of data for evaluation, and the remaining 80 percent for training([1-5] testing
and [5-25] training) while in the second iteration we use the second subset of 20 percent for
evaluation, and the remaining three subsets of the data for training([5-10] testing and [1-5 and
10-25] training), and so on.

Total instances: 25

Value of k : 5

No. Iteration Training set observations Testing set observations
1 [5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22 23 24] [61234]
2 [e 1 2 3 410 11 12 13 14 15 16 17 18 19 20 21 22 23 24] [56 7 8 9]

3 fe 1. 2 3 4 5 6 7 8 915 16 17 18 19 26 21 22 23 241 fie 11 12 13 141
v me mv mr mv mr mv ee me ee ea) L=v == me me ma

5 6 7 8 910 11 12 13 14 20 21 22 23 24] [15 16 17 18 19]
5 6 7 8 910 11 12 13 14 15 16 17 18 19] [20 21 22 23 24]

Comparison of train/test split to cross-validation

e Advantages of train/test split:
1. This runs K times faster than Leave One Out cross-validation because K-fold cross-
validation repeats the train/test split K-times.
2. Simpler to examine the detailed results of the testing process.
¢ Advantages of cross-validation:
1. More accurate estimate of out-of-sample accuracy.
2. More “efficient” use of data as every observation is used for both training and testing.
® Python code for k fold cross-validation-
# as required packages are not found.
# importing cross-validation from sklearn package.
from sklearn import cross_validation
# value of Kis 10.
data = cross_validation.KFold(len(train_set), n_folds=10, indices=False)

Interview Notes-
